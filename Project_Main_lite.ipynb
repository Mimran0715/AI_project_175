{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "\n",
    "NEG_DIRECTORY_PATH = './review_polarity/txt_sentoken/neg'\n",
    "POS_DIRECTORY_PATH = './review_polarity/txt_sentoken/pos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text Data\n",
    "\n",
    "1. iterate through negative and positive text files\n",
    "\n",
    "2. concat all lines per file to a single string\n",
    "\n",
    "3. create tensor dataset from list of strings\n",
    "\n",
    "4. label tensor dataset with 0 - negative | 1 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_sets = []\n",
    "\n",
    "neg_file_names = list(os.listdir(NEG_DIRECTORY_PATH))\n",
    "pos_file_names = list(os.listdir(POS_DIRECTORY_PATH))\n",
    "\n",
    "all_lines_list = []\n",
    "neg_lines_list = []\n",
    "for file_name in neg_file_names:\n",
    "  file = open(os.path.join(NEG_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  neg_lines_list.append(lines)\n",
    "  all_lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(neg_lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 0))\n",
    "labeled_data_sets.append(labeled_data_set)\n",
    "\n",
    "pos_lines_list = []\n",
    "for file_name in pos_file_names:\n",
    "  file = open(os.path.join(POS_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  pos_lines_list.append(lines)\n",
    "  all_lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(pos_lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 1))\n",
    "labeled_data_sets.append(labeled_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "1. Concat positive and negative reviews\n",
    "2. Double check size of full dataset\n",
    "3. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative labeled data len: 1000\n",
      "Negative data len: 1000\n",
      "Positive labeled data len: 1000\n",
      "Positive data len: 1000\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "\n",
    "neg_labeled_data = labeled_data_sets[0]\n",
    "pos_labeled_data = labeled_data_sets[1]\n",
    "print(\"Negative labeled data len:\", len(list(neg_labeled_data)))\n",
    "print(\"Negative data len:\", len(neg_file_names))\n",
    "print(\"Positive labeled data len:\", len(list(pos_labeled_data)))\n",
    "print(\"Positive data len:\", len(pos_file_names))\n",
    "\n",
    "pos_labeled_data = pos_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "neg_labeled_data = neg_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Samples: 2000\n",
      "Median num words per sample: 696.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import explore_data as ed\n",
    "\n",
    "print(\"# Samples:\", len(all_lines_list))\n",
    "print(\"Median num words per sample:\", ed.get_num_words_per_sample(all_lines_list))\n",
    "ed.plot_frequency_distribution_of_ngrams(all_lines_list,num_ngrams=20)\n",
    "ed.plot_sample_length_distribution(all_lines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=4046, shape=(), dtype=string, numpy=b'the premise of this movie is , well , pretty far-fetched . tom berenger plays shale , a mercenary who is temporarily out of work ( those fools at the cia have denied his existence just because he and his buddies botched a job in cuba ) . fortunately , his girl friend ( diane venora ) , a teacher at christopher columbus high school in miami , gets her knee cap broken by a disgruntled student , creating a job opening for shale as a substitute teacher . not telling his girl friend , who might object on pedagogical grounds , he creates a number of fake higher degrees for himself ( from yale , harvard , princeton , et al ) and begins his tenure as a high school teacher . the students ( junkies , drug dealers , gang members , sleazy sluts , ice-pick wielders . . . you get the picture ) don\\'t really take to him right away , so he hits one in the face with a can and breaks a few fingers . this gets their attention to a certain extent , so he tells them the story of the vietnam war : \" see , some homeboys from the north tried to muscle in on the turf of the homeboys from the south . \" oh yeah , now they can dig it ; the problem is just that nobody ever explained it properly before . but wait ! there are drugs being dealt in the school itself ! and behind the whole scheme , in cahoots with the head gang , the kod ( no , not \" cod \" , but \" knights of destruction \" . . . really ! ) , is none other than . . . the upright , ex-cop principal , played by the forgotten ghostbuster , ernie hudson ! so shale does what any good teacher would do . he gets his buddies together , they gather together a bunch of bazookas and other major weapons , explosives , and cool stuff like that , and they have a big showdown against the drug dealers and kod at the high school . ok , so the premise is not just far-fetched , it\\'s downright dumb . if this were a hong kong action comedy , we might just accept it , but it takes itself far too seriously to be truly fun . oh , it has its moments ; how one can truly hate a movie in which huge ( really huge ) amounts of cocaine are delivered in school busses ? and to be fair , it is almost never really boring , as the action is interrupted by only short sequences of actual story . but over all , this is pretty much a made-for-tv movie with more ( and bigger ) explosions and more foul language . in fact , it reminded me of \" miami vice \" without the production values , babes in skimpy bikinis , and pastels . if you can sneak into the theater without paying , go for it . otherwise , wait for video . the flying inkpot rating system : * wait for the tv2 broadcast .  * * a little creaky , but still better than staying at home with gotcha !  * * * pretty good , bring a friend .  * * * * amazing , potent stuff .  * * * * * perfection . see it twice . '>, <tf.Tensor: id=4047, shape=(), dtype=int32, numpy=0>)\n",
      "--------------\n",
      "(<tf.Tensor: id=4055, shape=(), dtype=string, numpy=b\"one of the last entries in the long-running carry on series , carry on behind is very similar to carry on camping in that it involves a group of holidaymakers descending on a 'caravan' site . professors anna vrooshka ( elke sommer ) and roland crump ( kenneth williams ) and a group of archaeology students stay in the caravan site owned by major leep ( kenneth connor ) so that they can explore the nearby roman settlement remains . anna has a little trouble understanding english and sometimes people get the wrong end of the stick : - for instance , when anna is asking for 'scrubbers for dirty caravan' , she means that she wants a scrubbing brush to clean the caravan ! arthur upmore ( bernard bresslaw ) and his wife linda ( patsy rowlands ) take her mother daphne barnes ( joan sims ) and her minah bird on holiday with them . mother-in-law jokes prevail . furthermore , the trouble that joe and norma baxter ( ian lavender and adrienne posta ) have with their large irish greyhound allows for some comical moments between the two families . fred ramsden ( windsor davies ) and ernie bragg ( jack douglas ) leave their wives ( liz fraser and patricia franklin ) behind for a fishing holiday . however , they have more in mind than fishing when they catch sight of two young girls , sandra ( carol hawkins ) and carol ( sherrie hewson ) . the story involves the disruption caused by the archaeological professors of the day-to-day running of the camp , the search for the minah bid and greyhound , the major , fred and ernie's desperate need of a woman , a misunderstanding leading to a striptease at the caravan park's pub , and a shock in store for daphne . the partnership between sommer and williams is very effective and amusing . this is what binds the movie together . joan sims stands out as the demanding mother-in-law , bernard bresslaw and patsy rowlands work well together as husband and wife , and carol hawkins and sherrie hewson are a welcome addition to the cast . on the other hand , there are low-key performances from kenneth connor and peter butterworth as barnes the handyman , and there are very poor performances by windsor davies and jack douglas who spend far too much time on the screen ! even though there is no real plot to speak of and the jokes are getting bluer due to the new scriptwriter dave freeman , there are enough truly comical moments and bright and breezy performances to lift this carry on above many of its predecessors . although the regular cast is depleted , the ones that remain show that they can still make a good carry on film . a relative flop at the cinema , this movie deserves a lot more recognition . \">, <tf.Tensor: id=4056, shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for ex in neg_labeled_data.take(1):\n",
    "  print(ex)\n",
    "print(\"--------------\")\n",
    "for ex in pos_labeled_data.take(1):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode Words\n",
    "\n",
    "1. Get unique vocabulary set among data\n",
    "2. Create encoder based on vocabulary set\n",
    "3. Encode data text -> int using vocabulary as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39696"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in neg_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "for text_tensor, _ in pos_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"one of the last entries in the long-running carry on series , carry on behind is very similar to carry on camping in that it involves a group of holidaymakers descending on a 'caravan' site . professors anna vrooshka ( elke sommer ) and roland crump ( kenneth williams ) and a group of archaeology students stay in the caravan site owned by major leep ( kenneth connor ) so that they can explore the nearby roman settlement remains . anna has a little trouble understanding english and sometimes people get the wrong end of the stick : - for instance , when anna is asking for 'scrubbers for dirty caravan' , she means that she wants a scrubbing brush to clean the caravan ! arthur upmore ( bernard bresslaw ) and his wife linda ( patsy rowlands ) take her mother daphne barnes ( joan sims ) and her minah bird on holiday with them . mother-in-law jokes prevail . furthermore , the trouble that joe and norma baxter ( ian lavender and adrienne posta ) have with their large irish greyhound allows for some comical moments between the two families . fred ramsden ( windsor davies ) and ernie bragg ( jack douglas ) leave their wives ( liz fraser and patricia franklin ) behind for a fishing holiday . however , they have more in mind than fishing when they catch sight of two young girls , sandra ( carol hawkins ) and carol ( sherrie hewson ) . the story involves the disruption caused by the archaeological professors of the day-to-day running of the camp , the search for the minah bid and greyhound , the major , fred and ernie's desperate need of a woman , a misunderstanding leading to a striptease at the caravan park's pub , and a shock in store for daphne . the partnership between sommer and williams is very effective and amusing . this is what binds the movie together . joan sims stands out as the demanding mother-in-law , bernard bresslaw and patsy rowlands work well together as husband and wife , and carol hawkins and sherrie hewson are a welcome addition to the cast . on the other hand , there are low-key performances from kenneth connor and peter butterworth as barnes the handyman , and there are very poor performances by windsor davies and jack douglas who spend far too much time on the screen ! even though there is no real plot to speak of and the jokes are getting bluer due to the new scriptwriter dave freeman , there are enough truly comical moments and bright and breezy performances to lift this carry on above many of its predecessors . although the regular cast is depleted , the ones that remain show that they can still make a good carry on film . a relative flop at the cinema , this movie deserves a lot more recognition . \"\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(pos_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38635, 3705, 6109, 27587, 24761, 37778, 6109, 2397, 3577, 7693, 8479, 9288, 7693, 8479, 12253, 3459, 36172, 28713, 12203, 7693, 8479, 1050, 37778, 11518, 18749, 6422, 36736, 33415, 3705, 7878, 24492, 8479, 36736, 26686, 13023, 5157, 17300, 21543, 12734, 34329, 33040, 31650, 4404, 15735, 5599, 33040, 36736, 33415, 3705, 26484, 26429, 12120, 37778, 6109, 26686, 13023, 27713, 19355, 13829, 8937, 15735, 15879, 1214, 11518, 20143, 34334, 33650, 6109, 35197, 33787, 34774, 17964, 17300, 11878, 36736, 31922, 18710, 20689, 31265, 33040, 36495, 32644, 6604, 6109, 38802, 19724, 3705, 6109, 6942, 19266, 36468, 9529, 17300, 3459, 3315, 19266, 31812, 19266, 12161, 26686, 18991, 13000, 11518, 18991, 4264, 36736, 15844, 37992, 12203, 39093, 6109, 26686, 22223, 31308, 39069, 38258, 33040, 27995, 11994, 6675, 14187, 15160, 10742, 33965, 15923, 27477, 38970, 14590, 37180, 33040, 33965, 13219, 23107, 8479, 5613, 2300, 17438, 15923, 37778, 10142, 1055, 35536, 34371, 6109, 18710, 11518, 39550, 33040, 32082, 4204, 27061, 22511, 33040, 36013, 30002, 1673, 2300, 33164, 28094, 32555, 6783, 24033, 19266, 24545, 26456, 19213, 14238, 6109, 13262, 11826, 13513, 23105, 15990, 7227, 33040, 10368, 27819, 21665, 23025, 16938, 33164, 34188, 10432, 22815, 33040, 26833, 31164, 12253, 19266, 36736, 21399, 5613, 20335, 20143, 1673, 24736, 37778, 10145, 16646, 21399, 9529, 20143, 12353, 37412, 3705, 13262, 10894, 34299, 4020, 4583, 14146, 33040, 4583, 38708, 30244, 6109, 35053, 6422, 6109, 7536, 12195, 19355, 6109, 22524, 5157, 3705, 6109, 11689, 12203, 11689, 3577, 3705, 6109, 32227, 6109, 18024, 19266, 6109, 13219, 2698, 33040, 6783, 6109, 13829, 13513, 33040, 10368, 6490, 15346, 24447, 3705, 36736, 6551, 36736, 6123, 14027, 12203, 36736, 15339, 25358, 6109, 26686, 21696, 6490, 8776, 33040, 36736, 26599, 37778, 28898, 19266, 27477, 6109, 31839, 14238, 34329, 33040, 5599, 3459, 36172, 20361, 33040, 28778, 4137, 3459, 25575, 35815, 6109, 1246, 26672, 14590, 37180, 34488, 32720, 1666, 6109, 38680, 15923, 37778, 10142, 39069, 38258, 33040, 14187, 15160, 28627, 25761, 26672, 1666, 16517, 33040, 11994, 33040, 4583, 14146, 33040, 38708, 30244, 349, 36736, 36940, 12151, 12203, 6109, 23228, 8479, 6109, 11644, 11225, 4389, 349, 13206, 5058, 30874, 3227, 15735, 15879, 33040, 3669, 28975, 1666, 38970, 6109, 20622, 33040, 4389, 349, 36172, 2214, 30874, 19355, 15990, 7227, 33040, 21665, 23025, 4779, 30761, 31111, 7559, 790, 27775, 8479, 6109, 30975, 22459, 23909, 4389, 3459, 12574, 4932, 4101, 12203, 21308, 3705, 33040, 6109, 1055, 349, 3801, 31391, 22103, 12203, 6109, 30586, 24596, 15964, 15577, 4389, 349, 34503, 84, 26456, 19213, 33040, 3552, 33040, 36733, 30874, 12203, 26938, 4137, 7693, 8479, 33383, 7740, 3705, 28965, 4589, 8085, 6109, 1523, 23228, 3459, 12080, 6109, 13636, 11518, 37573, 19338, 11518, 20143, 34334, 16417, 10614, 36736, 30718, 7693, 8479, 2887, 36736, 12720, 23783, 25358, 6109, 35593, 4137, 1246, 16983, 36736, 17801, 24736, 11363]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text, label = tf.py_function(encode, \n",
    "                                       inp=[text, label], \n",
    "                                       Tout=(tf.int64, tf.int32))\n",
    "\n",
    "  return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38635  3705  6109 27587 24761 37778  6109  2397  3577  7693  8479  9288\n",
      "  7693  8479 12253  3459 36172 28713 12203  7693  8479  1050 37778 11518\n",
      " 18749  6422 36736 33415  3705  7878 24492  8479 36736 26686 13023  5157\n",
      " 17300 21543 12734 34329 33040 31650  4404 15735  5599 33040 36736 33415\n",
      "  3705 26484 26429 12120 37778  6109 26686 13023 27713 19355 13829  8937\n",
      " 15735 15879  1214 11518 20143 34334 33650  6109 35197 33787 34774 17964\n",
      " 17300 11878 36736 31922 18710 20689 31265 33040 36495 32644  6604  6109\n",
      " 38802 19724  3705  6109  6942 19266 36468  9529 17300  3459  3315 19266\n",
      " 31812 19266 12161 26686 18991 13000 11518 18991  4264 36736 15844 37992\n",
      " 12203 39093  6109 26686 22223 31308 39069 38258 33040 27995 11994  6675\n",
      " 14187 15160 10742 33965 15923 27477 38970 14590 37180 33040 33965 13219\n",
      " 23107  8479  5613  2300 17438 15923 37778 10142  1055 35536 34371  6109\n",
      " 18710 11518 39550 33040 32082  4204 27061 22511 33040 36013 30002  1673\n",
      "  2300 33164 28094 32555  6783 24033 19266 24545 26456 19213 14238  6109\n",
      " 13262 11826 13513 23105 15990  7227 33040 10368 27819 21665 23025 16938\n",
      " 33164 34188 10432 22815 33040 26833 31164 12253 19266 36736 21399  5613\n",
      " 20335 20143  1673 24736 37778 10145 16646 21399  9529 20143 12353 37412\n",
      "  3705 13262 10894 34299  4020  4583 14146 33040  4583 38708 30244  6109\n",
      " 35053  6422  6109  7536 12195 19355  6109 22524  5157  3705  6109 11689\n",
      " 12203 11689  3577  3705  6109 32227  6109 18024 19266  6109 13219  2698\n",
      " 33040  6783  6109 13829 13513 33040 10368  6490 15346 24447  3705 36736\n",
      "  6551 36736  6123 14027 12203 36736 15339 25358  6109 26686 21696  6490\n",
      "  8776 33040 36736 26599 37778 28898 19266 27477  6109 31839 14238 34329\n",
      " 33040  5599  3459 36172 20361 33040 28778  4137  3459 25575 35815  6109\n",
      "  1246 26672 14590 37180 34488 32720  1666  6109 38680 15923 37778 10142\n",
      " 39069 38258 33040 14187 15160 28627 25761 26672  1666 16517 33040 11994\n",
      " 33040  4583 14146 33040 38708 30244   349 36736 36940 12151 12203  6109\n",
      " 23228  8479  6109 11644 11225  4389   349 13206  5058 30874  3227 15735\n",
      " 15879 33040  3669 28975  1666 38970  6109 20622 33040  4389   349 36172\n",
      "  2214 30874 19355 15990  7227 33040 21665 23025  4779 30761 31111  7559\n",
      "   790 27775  8479  6109 30975 22459 23909  4389  3459 12574  4932  4101\n",
      " 12203 21308  3705 33040  6109  1055   349  3801 31391 22103 12203  6109\n",
      " 30586 24596 15964 15577  4389   349 34503    84 26456 19213 33040  3552\n",
      " 33040 36733 30874 12203 26938  4137  7693  8479 33383  7740  3705 28965\n",
      "  4589  8085  6109  1523 23228  3459 12080  6109 13636 11518 37573 19338\n",
      " 11518 20143 34334 16417 10614 36736 30718  7693  8479  2887 36736 12720\n",
      " 23783 25358  6109 35593  4137  1246 16983 36736 17801 24736 11363]\n",
      "[ 6109 28479  3705  4137  1246  3459 25761 24919 31111 33170 29410 17793\n",
      " 33567  4541 36736 27010  4779  3459 38158 32720  3705 28627 17138  1426\n",
      " 25358  6109 25012  1673 27300 27995 25892 32025 15244 38423 33040 27995\n",
      " 22002  9759 36736 10545 37778  4263  2529 27995 12670 10012 16113  5467\n",
      " 36736 23849 25358 11293 21064  6363  7789 37778 24587 24088 33965 25935\n",
      "  3286 21450 19355 36736 28151  8265 12201 36736 10545 23430 19266  4541\n",
      "  1666 36736 16805 23849 11101 30057 27995 12670 10012  4779 28027 24866\n",
      "  8479 10712 24423 38423  6676 36736 20534  3705 11442 37605 13752 19266\n",
      " 38639  3227 39427 32277 29564 38282 37828 33040 19928 27995 12732  1666\n",
      " 36736  6363  7789 23849  6109 26429 38701 33108 26749  1237  3879 32912\n",
      "  4172 11501 15629  8723 16371  6604  6109 36732  1941 39693 31266 10742\n",
      " 12203 17839 35160 24018  1214 38423 31914 38635 37778  6109 13649  2300\n",
      " 36736 34334 33040 16369 36736 35858 18128  4137 24088 33164 16788 12203\n",
      " 36736 11883  8296  1214 38423 30957 17438  6109 35053  3705  6109 28607\n",
      "  6282 19549 24545 23157  3227  6109  4335 28587 12203   832 37778  8479\n",
      "  6109   629  3705  6109 23157  3227  6109   776  8929  9558 20846 20143\n",
      " 34334  2681 18749  6109   111  3459 32025 11518 16354  3713  4032 18749\n",
      " 30131 23846 19232 13433  4389   349  8765 15232  2308 37778  6109  7789\n",
      " 26480 33040 12253  6109 15557 11802 37778 36691  2300  6109 39082  1237\n",
      "  6109  7454 12574 11101 11228 19232  1182  3705  5463 31266  3459 38195\n",
      " 11644 16646  6109 12336 24230 25246 27233  1936 19355  6109  3497 28332\n",
      " 10368 26249  1214  4541 34249 25575 15696 30718 23849 20915 35605 38423\n",
      " 24088 27995 22002 26672 20143 20700 26672 36736 28187  3705 21417 33040\n",
      " 11644 13829 39387 28348 33040  2122 18726 34072 11518 33040 20143  1673\n",
      " 36736 35800 20381  3991  6109 33108 26749 33040  7454 25358  6109  6363\n",
      "  7789 15329  1214  6109 28479  3459 11101 32025 31111 33170 18749  6490\n",
      " 35444 39537 23086  4137 21509 36736 38092 35487 17472 28191 32262 28027\n",
      " 32025 13112 18749 19232 18749 21524 26480 31111  7559 21664 12203 12804\n",
      "    84 36721  8929 18749 11878 28965 19213 27719 38635 34334    84 11330\n",
      " 36736  1246 37778 11941  7561 31266  7561 13729  3705 19659   349 39165\n",
      " 37778  7789 38390 33040 12203 12804 19959 18749  3459 17021 20002 31266\n",
      " 29058  1666  6109 17472  3459 26059 19355 26949 17920  2523  3705 12819\n",
      " 35053 19232 24049  4150  4137  3459 24919   790 36736 38240 19266 30991\n",
      "  1246  2300 24736 33040 26663 27583 33040 24736 20852  1543 37778 38553\n",
      " 18749 19716 38656  3705 24587 23745 20414  6109  7463 13186 16906 37778\n",
      " 26049 39514 33040 17820 23086 16371 34334 27913 15693  6109 24083 20414\n",
      " 32173 21680 19266 18749 36650 13433 19266 27006  6109 34553 29450  2628\n",
      " 27523 13433 19266  6109  2506  4671 36736 31922 19199 19232 16417 11176\n",
      " 16646 12459 25358 33343  2300 26975 24919 30718 22718 36736 10012 18430\n",
      " 33338 18726 10779 19549 18749 13605]\n"
     ]
    }
   ],
   "source": [
    "pos_encoded_data = pos_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(pos_encoded_data))[0].numpy()\n",
    "print(example_encoding)\n",
    "\n",
    "neg_encoded_data = neg_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(neg_encoded_data))[0].numpy()\n",
    "print(example_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "TRAIN_AMT = 0.8\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "take_size = math.ceil(len(list(neg_encoded_data)) * (1 - TRAIN_AMT))\n",
    "print(take_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "400\n",
      "320\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "train_data_pos = pos_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "train_data_neg = neg_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "\n",
    "all_labeled_train_data = train_data_pos.concatenate(train_data_neg).shuffle(BUFFER_SIZE * 2)\n",
    "train_data = all_labeled_train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "\n",
    "test_data_pos = pos_encoded_data.take(take_size)\n",
    "test_data_neg = neg_encoded_data.take(take_size)\n",
    "all_labeled_test_data = test_data_pos.concatenate(test_data_neg).shuffle(BUFFER_SIZE)\n",
    "test_data = all_labeled_test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "\n",
    "print(len(list(train_data_pos)) + len(list(train_data_neg)))\n",
    "print(len(list(test_data_pos)) + len(list(test_data_neg)))\n",
    "train_data_size = len(list(train_data))\n",
    "test_data_size = len(list(test_data))\n",
    "print(train_data_size)\n",
    "print(test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=38402, shape=(5, 931), dtype=int64, numpy=\n",
      "array([[30718, 24164,   349, ...,     0,     0,     0],\n",
      "       [ 7195, 19355, 19555, ..., 20544,  6109, 27314],\n",
      "       [ 1607, 21642,  2887, ...,     0,     0,     0],\n",
      "       [ 4188, 32581,  6091, ...,     0,     0,     0],\n",
      "       [ 6109, 13982, 35053, ...,     0,     0,     0]])>, <tf.Tensor: id=38403, shape=(5,), dtype=int32, numpy=array([1, 0, 0, 0, 0], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "s_train = set()\n",
    "for text, labels in train_data:\n",
    "    s_train.add(labels[0].numpy())\n",
    "    \n",
    "s_test = set()\n",
    "for text, labels in test_data:\n",
    "    s_test.add(labels[0].numpy())\n",
    "print(s_train)\n",
    "print(s_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 3033 32219  3517 ...     0     0     0]\n",
      " [ 4515 18827  5179 ...     0     0     0]\n",
      " [18749  7929 34072 ...  6363  5170  4160]\n",
      " [30570  7422 23207 ...     0     0     0]\n",
      " [ 1214  4188 21243 ...     0     0     0]], shape=(5, 1235), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3033, 32219,  3517, ...,     0,     0,     0]), 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "print(sample_text)\n",
    "sample_text[0].numpy(), sample_labels[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1 # we added 0 for the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "We are going to create 5 word embeddings:\n",
    "\n",
    "1. Bag of words encoding\n",
    "2. Manually trained word embedding on data vocabulary set (Continuous bag of words model)\n",
    "3. Pre-trained Glove 100-dimension embedding\n",
    "4. Pre-trained Glove 300-dimension embedding\n",
    "5. Pre-trained Word2Vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of words\n",
    "\n",
    "Here's an example of how we can use tf.one_hot and bitwise or to create a bag of words encoding for our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=49314, shape=(4, 10), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.ops import bitwise_ops\n",
    "test_vocab = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "test_sentences = [[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9],\n",
    "                  [0, 5, 1]]\n",
    "depth = len(test_vocab)\n",
    "\n",
    "def bitwise_or_multiple(tensors):\n",
    "    res = tensors[0]\n",
    "    for i in range(1, len(tensors)):\n",
    "        res = bitwise_ops.bitwise_or(res, tensors[i])\n",
    "    \n",
    "    return res\n",
    "\n",
    "bag_of_words_list = []\n",
    "for sentence in tf.one_hot(test_sentences, depth):\n",
    "    print(sentence)\n",
    "    bag_of_words_list.append(tf.cast(bitwise_or_multiple(tf.cast(sentence, tf.uint32)), tf.float32))\n",
    "\n",
    "bag_of_words = tf.stack(bag_of_words_list)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Manually Trained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          635168    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 635,457\n",
      "Trainable params: 635,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 16s 51ms/step - loss: 0.6907 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.6624 - accuracy: 0.5038 - val_loss: 0.6391 - val_accuracy: 0.5075\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.5424 - accuracy: 0.6856 - val_loss: 0.5256 - val_accuracy: 0.7425\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3613 - accuracy: 0.8594 - val_loss: 0.4306 - val_accuracy: 0.7650\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2219 - accuracy: 0.9362 - val_loss: 0.3760 - val_accuracy: 0.8025\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.1377 - accuracy: 0.9706 - val_loss: 0.3287 - val_accuracy: 0.8275\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0863 - accuracy: 0.9900 - val_loss: 0.3276 - val_accuracy: 0.8300\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0617 - accuracy: 0.9925 - val_loss: 0.3099 - val_accuracy: 0.8500\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0415 - accuracy: 0.9956 - val_loss: 0.3374 - val_accuracy: 0.8600\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 13s 39ms/step - loss: 0.0305 - accuracy: 0.9981 - val_loss: 0.3152 - val_accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39698, 16)\n",
      "[-0.01200673 -0.04377279 -0.01682586 -0.05665943 -0.0985653   0.1759432\n",
      " -0.0046955  -0.05334025 -0.07617784  0.03212772  0.10440928 -0.06768385\n",
      "  0.03056481 -0.0538655   0.01223458  0.02964083]\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights_manual = e.get_weights()[0]\n",
    "print(weights_manual.shape) # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "print(weights_manual[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Final Model and Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "320/320 [==============================] - 13s 41ms/step - loss: 0.3064 - accuracy: 0.8138 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/8\n",
      "320/320 [==============================] - 9s 27ms/step - loss: 0.0214 - accuracy: 0.9994 - val_loss: 0.3259 - val_accuracy: 0.8525\n",
      "Epoch 3/8\n",
      "320/320 [==============================] - 9s 27ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.3700 - val_accuracy: 0.8575\n",
      "Epoch 4/8\n",
      "320/320 [==============================] - 9s 27ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3693 - val_accuracy: 0.8600\n",
      "Epoch 5/8\n",
      "320/320 [==============================] - 9s 27ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.4240 - val_accuracy: 0.8600\n",
      "Epoch 6/8\n",
      "320/320 [==============================] - 9s 28ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.4446 - val_accuracy: 0.8575\n",
      "Epoch 7/8\n",
      "320/320 [==============================] - 9s 28ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4967 - val_accuracy: 0.8550\n",
      "Epoch 8/8\n",
      "320/320 [==============================] - 9s 28ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.5978 - val_accuracy: 0.8525\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.5933 - accuracy: 0.8525\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(input_dim=encoder.vocab_size, \n",
    "                                    output_dim=embedding_dim, \n",
    "                                    weights=[weights_manual],\n",
    "                                    mask_zero=True,\n",
    "                                    trainable=False))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=64,\n",
    "                          kernel_size=5,\n",
    "                          activation='relu',\n",
    "                                ))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dropout(rate=0))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "\n",
    "# Compile and train model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, epochs=8, validation_data=test_data)\n",
    "eval_loss, eval_acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_review = \"This movie is a gosh darn masterpiece. It will make you belly laugh, \\\n",
    "            it will chill you to the bone, and it will make you shed a tear. This \\\n",
    "            movie will stay with you long after the credits are over. If you plan \\\n",
    "            on watching this movie, AVOID SPOILERS AT ALL COSTS.\"\n",
    "\n",
    "neg_review_1 = \"Saving Christmas needed saving...and by that, I mean it should have \\\n",
    "been mercy killed. the acting is horrendous and it's story is only a reminder that \\\n",
    "anyone that paid to see it wasted their money. Even if you are a die-hard Christian, \\\n",
    "please do not see this movie, it's one of the worst movies you could ever watch ever, \\\n",
    "and considering stuff like The Room and Birdemic exists, That's saying A lot. oh, and \\\n",
    "that thing Kirk Cameron's doing to try to boost the rating on Rotten Tomatoes: Cameron,\\\n",
    "you should know the Bible verse 'Thou shalt not bear false witness' it's one of the 10 \\\n",
    "commandments. That's all I have to say on this matter, Don't watch the film. I mean \\\n",
    "Seriously Cameron, you should know better.\"\n",
    "\n",
    "neg_review_2 = \"There is no script. Action poor. Acting Poor. A strict no! Pleas save \\\n",
    "find your money on this one! I wouldn't even rate if possible. Worst ever music! No heads \\\n",
    "or tails!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie is a gosh darn masterpiece. It will make you belly laugh,             it will chill you to the bone, and it will make you shed a tear. This             movie will stay with you long after the credits are over. If you plan             on watching this movie, AVOID SPOILERS AT ALL COSTS.\n",
      "Class: 1\n",
      "Encoded: [39697, 1246, 3459, 36736, 21134, 38181, 36855, 39697, 22613, 10614, 16371, 30159, 26688, 18749, 22613, 26613, 16371, 12203, 6109, 37840, 33040, 18749, 22613, 10614, 16371, 402, 36736, 38075, 39697, 1246, 22613, 12120, 2300, 16371, 2397, 14292, 6109, 31432, 349, 24049, 39697, 16371, 20410, 8479, 32833, 4137, 1246, 39697, 39697, 39697, 39697, 39697]\n",
      "Prediction: [[1]]\n",
      "----------\n",
      "Review: Saving Christmas needed saving...and by that, I mean it should have been mercy killed. the acting is horrendous and it's story is only a reminder that anyone that paid to see it wasted their money. Even if you are a die-hard Christian, please do not see this movie, it's one of the worst movies you could ever watch ever, and considering stuff like The Room and Birdemic exists, That's saying A lot. oh, and that thing Kirk Cameron's doing to try to boost the rating on Rotten Tomatoes: Cameron,you should know the Bible verse 'Thou shalt not bear false witness' it's one of the 10 commandments. That's all I have to say on this matter, Don't watch the film. I mean Seriously Cameron, you should know better.\n",
      "Class: 0\n",
      "Encoded: [39697, 39697, 36103, 966, 33040, 19355, 11518, 39697, 18054, 18749, 772, 1673, 26301, 38008, 7943, 6109, 29822, 3459, 5085, 33040, 18749, 6490, 35053, 3459, 26949, 36736, 1912, 11518, 31809, 11518, 23486, 12203, 19549, 18749, 17741, 33164, 24595, 39697, 23086, 16371, 349, 36736, 719, 7590, 39697, 18427, 35605, 11101, 19549, 4137, 1246, 18749, 6490, 38635, 3705, 6109, 32393, 23727, 16371, 5573, 3713, 37409, 3713, 33040, 21386, 18726, 34072, 39697, 39697, 33040, 39697, 19334, 39697, 6490, 5611, 39697, 17801, 8929, 33040, 11518, 3049, 39697, 39697, 6490, 10961, 12203, 19221, 12203, 36199, 6109, 2628, 8479, 39697, 39697, 39697, 16371, 772, 8428, 6109, 39697, 17017, 39697, 32408, 11101, 38972, 36116, 12723, 18749, 6490, 38635, 3705, 6109, 23777, 14377, 39697, 6490, 4150, 39697, 1673, 12203, 13709, 8479, 4137, 31016, 39697, 39693, 37409, 6109, 2887, 39697, 18054, 39697, 39697, 16371, 772, 8428, 11176]\n",
      "Prediction: [[0]]\n",
      "----------\n",
      "Review: There is no script. Action poor. Acting Poor. A strict no! Pleas save find your money on this one! I wouldn't even rate if possible. Worst ever music! No heads or tails!\n",
      "Class: 0\n",
      "Encoded: [39697, 3459, 12574, 27074, 39697, 2214, 39697, 39697, 39697, 35691, 12574, 39697, 4630, 25985, 7418, 24595, 8479, 4137, 38635, 39697, 8043, 39693, 22459, 23117, 23086, 1243, 39697, 3713, 32914, 39697, 29323, 27526, 35728]\n",
      "Prediction: [[0]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for review in [(pos_review, 1), (neg_review_1, 0),(neg_review_2, 0)]:\n",
    "    print(\"Review:\",review[0])\n",
    "    print(\"Class:\", review[1])\n",
    "    \n",
    "    encoded = encoder.encode(review[0])\n",
    "    print(\"Encoded:\", encoded)\n",
    "    \n",
    "    print(\"Prediction:\", model.predict_classes([encoded]))\n",
    "    print(\"----------\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
