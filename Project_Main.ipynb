{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "\n",
    "NEG_DIRECTORY_PATH = './review_polarity/txt_sentoken/neg'\n",
    "POS_DIRECTORY_PATH = './review_polarity/txt_sentoken/pos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text Data\n",
    "\n",
    "1. iterate through negative and positive text files\n",
    "\n",
    "2. concat all lines per file to a single string\n",
    "\n",
    "3. create tensor dataset from list of strings\n",
    "\n",
    "4. label tensor dataset with 0 - negative | 1 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_sets = []\n",
    "\n",
    "neg_file_names = list(os.listdir(NEG_DIRECTORY_PATH))\n",
    "pos_file_names = list(os.listdir(POS_DIRECTORY_PATH))\n",
    "\n",
    "lines_list = []\n",
    "for file_name in neg_file_names:\n",
    "  file = open(os.path.join(NEG_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 0))\n",
    "labeled_data_sets.append(labeled_data_set)\n",
    "\n",
    "lines_list = []\n",
    "for file_name in pos_file_names:\n",
    "  file = open(os.path.join(POS_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 1))\n",
    "labeled_data_sets.append(labeled_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "1. Concat positive and negative reviews\n",
    "2. Double check size of full dataset\n",
    "3. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative labeled data len: 1000\n",
      "Negative data len: 1000\n",
      "Positive labeled data len: 1000\n",
      "Positive data len: 1000\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "\n",
    "neg_labeled_data = labeled_data_sets[0]\n",
    "pos_labeled_data = labeled_data_sets[1]\n",
    "print(\"Negative labeled data len:\", len(list(neg_labeled_data)))\n",
    "print(\"Negative data len:\", len(neg_file_names))\n",
    "print(\"Positive labeled data len:\", len(list(pos_labeled_data)))\n",
    "print(\"Positive data len:\", len(pos_file_names))\n",
    "\n",
    "pos_labeled_data = pos_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "neg_labeled_data = neg_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'seen december 2 , 1997 at 6 : 50 p . m . at the glenwood movieplex cinemas ( oneida , ny ) , theater #3 , by myself for free ( free pass ) . [theater rating : * * * : good seats , sound , and picture] there are many philosophies as to why we are so fascinated with cartoons . they provide a method of total escapism in which anything will work within their context , from the outrageous slapstick of looney tunes to the intensity of japanimation . watching \" flubber \" really clinched this idea for me , because it\\'s just a live action cartoon that presents itself as a regular comedy . it proves how painfully unfunny all those gags and slapstick would be in reality , and how important it is to actually have a story . the film wastes no time in establishing its lighthearted , cartoony atmosphere . we meet medfield college chemistry professor phillip brainard ( williams ) , the typical , supposedly likable mad scientist . within the first 10 minutes we get at least a half dozen jokes about how forgetful he is . phillip\\'s memory loss seems less like a cartoony gag and more like a real case of alzhiemer\\'s disease - isn\\'t this rather lowbrow comedy ? he starts teaching chemistry after walking in on a nude figure drawing class ( an inappropriate joke for a kids\\' movie if i ever saw one ) . we learn he has stood his up fiancee , sara ( harden ) , twice at the altar simply because he\\'s forgotten . sara claims to love him , but says if he forgets the wedding again she\\'ll stop loving him ( a sign of childish attitude the film denies it has ) . but even cartoons must have conflicts to resolve , and it\\'s no surprise the major conflicts here are related . christopher mcdonald does and even worse version of his usual villain shtick as wilson croft , a scientist who not only wants to steal phillip\\'s ideas , but his woman too . what\\'s worse is that sara actually seems interested in him ! wilson works for a rival college that wants to buy out medfield , which is going broke , unless phillip can invent something to save the day . one of the first rules of filmmaking is to make sure the title has something to do with the film . the flubber is played for gags after it is introduced , but is all but forgotten about thereafter . it\\'s obvious phillip is going to accidentally invent flubber , a flying rubber compound that yields tremendous energy , and the promotions make the flubber look realistic and funny . unfortunately , the flubber , like all the special effects in this film , looks very fake and unconvincing . flubber also seems to have intelligence and a personality , and by the time it performs a song and dance routine , you give up wondering how life could spontaneously come to an inanimate object . most of the film wanders aimlessly as it relies on the flubber to make for the comedy ( since phillip\\'s absent-mindedness is forgotten about ) and somehow push the story along . but it\\'s clear how weak this premise is from the get-go and the film just gets worse . all the jokes involve people getting hit by fast-moving , flubber-powered objects including : bowling balls ; flying cars that don\\'t fly well ; and a basketball team capable of jumping 100 feet into the air . it\\'s all presented with complete logic , and no one believes flubber exists even when they see it with their own eyes . during the basketball scene , the coach for the rival team actually says , \" i think they might be cheating , \" to which the referee replies , \" there\\'s no rules about jumping too high . \"  \" predictable \" doesn\\'t begin to describe the motions the film goes through , especially after the halfway point . and it\\'s not how terribly contrived the plot is , it\\'s the way it\\'s broken down scene-by-scene , with absolutely no transition . not only that , but all the actors here , especially williams , seem bored to tears . i\\'d say 80 percent of all the scenes were shot with a bluescreen , so it\\'s almost forgivable when you realize they\\'re talking to nothing and interacting with nothing . it\\'s safe to say everything that could be bad about \" flubber \" is . the only original element the film has is the world\\'s first romance between a machine and a human . but this aspect is quite twisted if you think about it , and , like the rest of the film , you won\\'t . '>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'clue is an unfairly ignored comedy , very similar to 1976\\'s murder by death . this big screen version of the classic board game ( what\\'s next . . . chutes and ladders : the motion picture ? ) is filled with slapstick antics and silly dialogue . the plot , for what it\\'s worth , has all the characters from the game ( in this case , the names from the game are used as aliases ) meeting in an isolated mansion to confront mr . boddy ( lee ving ) , the man who\\'s been blackmailing them all . when he turns up dead , everyone ( including the audience ) must figure out whodunnit . . . and in what room , and with what object . while not as witty as neil simon\\'s murder by death , clue definitely has it moments . it has so many moments in fact that i use a lot of the lines from the film when i\\'m joking around with my friends . to this day , whenever someone says the phrase \" well , to make a long story short \" i have the follow up phrase \" too late \" ready to go . the cast ( all very good comedic talents ) play well off one another , while the late madeline kahn ( as the dark and sultry mrs . white ) sometimes steals the film away from the rest . and colleen camp , as the french maid yvette , displays some of her natural talents as well . clue is available on dvd from paramount home video . it includes the film in its original theatrical aspect ratio of 1 . 85 : 1 ( and is enhanced for 16x9 televisions ) and features the original theatrical trailer . a french language audio track is also available . the trailer holds up well considering most previews from that time do not , and it even includes a scene not in the film itself ( a scene that should have been in the film , as it\\'s a good joke ) . also , the trailer is scored to the music from airplane ! , which was an interesting choice . when clue played in theaters , it ran with the gimmick of three different endings . if you wanted to see all three , you had to go to the movies three separate times . so when the film was released on home video , rather than releasing three separate videos , all three endings were included on one tape . . . the first two endings being \" what if ? \" endings and the third ending being the actual ending . now for the dvd release you get two choices . you can watch the film as it was presented on home video or you can select to watch it with one of the three endings randomly chosen for you .  ( note : there\\'s an easter egg hidden in the disc pertaining to this . after watching the film with a randomly selected ending , when you return to the menu screen you will be able to highlight the large magnifying glass and select it . when you do , a secret menu opens , allowing you to watch any of the three endings by themselves . ) now while i applaud the effort of paramount here , as they clearly tried to do something special with the disc , it just doesn\\'t work well . first , why not give the viewer the choice of what ending we want to watch ? maybe someone like myself who\\'s seen the movie hundreds of times would like to sit down and show it to someone with the second ending only . i\\'m no technical dvd expert , but i can\\'t imagine that being too hard to accomplish . secondly , the delay between when the film itself ends and one of the endings begins is too long and too obvious , thereby becoming a distraction right when you definitely don\\'t need a distraction . finally , the back of the dvd case states \" and now , with this special dvd version , you can see all 3 surprise endings \" . i have no idea why they would word it that way , since that\\'s not a special feature in the slightest . the video has been out for fifteen years now ( good lord , has it been that long ? ) and it\\'s played on television with all three endings all the time . these are merely minor complaints however , seeing that i\\'ve watched the home video version hundreds of times and have no problem watching the film as such on the dvd . the picture and sound are wonderfully improved over my worn out vhs copy , and i\\'m thrilled that paramount agrees with me that clue is a film worthy of being preserved on this great digital format . [pg] '>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for ex in neg_labeled_data.take(1):\n",
    "  print(ex)\n",
    "print(\"--------------\")\n",
    "for ex in pos_labeled_data.take(1):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode Words\n",
    "\n",
    "1. Get unique vocabulary set among data\n",
    "2. Create encoder based on vocabulary set\n",
    "3. Encode data text -> int using vocabulary as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39696"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in neg_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "for text_tensor, _ in pos_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'clue is an unfairly ignored comedy , very similar to 1976\\'s murder by death . this big screen version of the classic board game ( what\\'s next . . . chutes and ladders : the motion picture ? ) is filled with slapstick antics and silly dialogue . the plot , for what it\\'s worth , has all the characters from the game ( in this case , the names from the game are used as aliases ) meeting in an isolated mansion to confront mr . boddy ( lee ving ) , the man who\\'s been blackmailing them all . when he turns up dead , everyone ( including the audience ) must figure out whodunnit . . . and in what room , and with what object . while not as witty as neil simon\\'s murder by death , clue definitely has it moments . it has so many moments in fact that i use a lot of the lines from the film when i\\'m joking around with my friends . to this day , whenever someone says the phrase \" well , to make a long story short \" i have the follow up phrase \" too late \" ready to go . the cast ( all very good comedic talents ) play well off one another , while the late madeline kahn ( as the dark and sultry mrs . white ) sometimes steals the film away from the rest . and colleen camp , as the french maid yvette , displays some of her natural talents as well . clue is available on dvd from paramount home video . it includes the film in its original theatrical aspect ratio of 1 . 85 : 1 ( and is enhanced for 16x9 televisions ) and features the original theatrical trailer . a french language audio track is also available . the trailer holds up well considering most previews from that time do not , and it even includes a scene not in the film itself ( a scene that should have been in the film , as it\\'s a good joke ) . also , the trailer is scored to the music from airplane ! , which was an interesting choice . when clue played in theaters , it ran with the gimmick of three different endings . if you wanted to see all three , you had to go to the movies three separate times . so when the film was released on home video , rather than releasing three separate videos , all three endings were included on one tape . . . the first two endings being \" what if ? \" endings and the third ending being the actual ending . now for the dvd release you get two choices . you can watch the film as it was presented on home video or you can select to watch it with one of the three endings randomly chosen for you .  ( note : there\\'s an easter egg hidden in the disc pertaining to this . after watching the film with a randomly selected ending , when you return to the menu screen you will be able to highlight the large magnifying glass and select it . when you do , a secret menu opens , allowing you to watch any of the three endings by themselves . ) now while i applaud the effort of paramount here , as they clearly tried to do something special with the disc , it just doesn\\'t work well . first , why not give the viewer the choice of what ending we want to watch ? maybe someone like myself who\\'s seen the movie hundreds of times would like to sit down and show it to someone with the second ending only . i\\'m no technical dvd expert , but i can\\'t imagine that being too hard to accomplish . secondly , the delay between when the film itself ends and one of the endings begins is too long and too obvious , thereby becoming a distraction right when you definitely don\\'t need a distraction . finally , the back of the dvd case states \" and now , with this special dvd version , you can see all 3 surprise endings \" . i have no idea why they would word it that way , since that\\'s not a special feature in the slightest . the video has been out for fifteen years now ( good lord , has it been that long ? ) and it\\'s played on television with all three endings all the time . these are merely minor complaints however , seeing that i\\'ve watched the home video version hundreds of times and have no problem watching the film as such on the dvd . the picture and sound are wonderfully improved over my worn out vhs copy , and i\\'m thrilled that paramount agrees with me that clue is a film worthy of being preserved on this great digital format . [pg] '\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(pos_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21508, 16125, 29981, 6207, 23693, 292, 31415, 33530, 3730, 6726, 33923, 21229, 31746, 27701, 5978, 29141, 11487, 26417, 4477, 30687, 28934, 5634, 29489, 39456, 33923, 39425, 11993, 13083, 9935, 30687, 23710, 25757, 16125, 30781, 23373, 25400, 1190, 13083, 10301, 734, 30687, 39065, 19557, 39456, 12625, 33923, 15004, 23136, 34663, 30687, 6807, 33622, 30687, 29489, 37023, 5978, 2782, 30687, 19229, 33622, 30687, 29489, 11366, 23037, 17908, 304, 37008, 37023, 29981, 32442, 23183, 3730, 30927, 11733, 15199, 36497, 17313, 30687, 31515, 33567, 33923, 28957, 25546, 7273, 34663, 20055, 35984, 2238, 38169, 39507, 35907, 4282, 30687, 9491, 22603, 22320, 1027, 22183, 13083, 37023, 39456, 33847, 13083, 23373, 39456, 9177, 25574, 3050, 17908, 23150, 17908, 17429, 20298, 33923, 21229, 31746, 27701, 21508, 26287, 23136, 12625, 17995, 12625, 23136, 7378, 14609, 17995, 37023, 10073, 27316, 34805, 2484, 26578, 23173, 4477, 30687, 19284, 33622, 30687, 4453, 20055, 34805, 12232, 34077, 11217, 23373, 32903, 24422, 3730, 5978, 30300, 21943, 9119, 17422, 30687, 11837, 7171, 3730, 6873, 26578, 22153, 26812, 20966, 34805, 17407, 30687, 14116, 38169, 11837, 25146, 31599, 12497, 3730, 23786, 30687, 21917, 34663, 31415, 8396, 13284, 33752, 12140, 7171, 29352, 26042, 35415, 25574, 30687, 31599, 15059, 14253, 17908, 30687, 6478, 13083, 22986, 28417, 13588, 19238, 13067, 30687, 4453, 16341, 33622, 30687, 23905, 13083, 14702, 27136, 17908, 30687, 33122, 20050, 21688, 38753, 21848, 4477, 3949, 22708, 33752, 17908, 7171, 21508, 16125, 23486, 9129, 4792, 33622, 19230, 581, 26377, 12625, 19360, 30687, 4453, 37023, 15631, 14269, 35981, 32356, 35783, 4477, 2940, 29332, 2940, 13083, 16125, 4727, 19557, 29713, 39270, 13083, 3205, 30687, 14269, 35981, 17141, 26578, 33122, 32068, 190, 38483, 16125, 11277, 23486, 30687, 17141, 5279, 38169, 7171, 16105, 14474, 20960, 33622, 27316, 29785, 22189, 3050, 13083, 12625, 38713, 19360, 26578, 25262, 3050, 37023, 30687, 4453, 7760, 26578, 25262, 27316, 9092, 17407, 28957, 37023, 30687, 4453, 17908, 12625, 33923, 26578, 8396, 7427, 11277, 30687, 17141, 16125, 30579, 3730, 30687, 33168, 33622, 39206, 28126, 17472, 29981, 11240, 25502, 20055, 21508, 17129, 37023, 22694, 12625, 16169, 23373, 30687, 4893, 4477, 33279, 27409, 26083, 29222, 19759, 31050, 3730, 38774, 34663, 33279, 19759, 13910, 3730, 23786, 3730, 30687, 22993, 33279, 10398, 29528, 7378, 20055, 30687, 4453, 17472, 24948, 9129, 581, 26377, 17884, 4934, 11268, 33279, 10398, 29602, 34663, 33279, 26083, 1405, 4449, 9129, 26042, 9018, 30687, 11960, 4560, 26083, 26451, 39456, 29222, 26083, 13083, 30687, 9788, 12470, 26451, 30687, 26468, 12470, 32742, 19557, 30687, 4792, 1770, 19759, 20287, 4560, 36262, 19759, 14914, 15010, 30687, 4453, 17908, 12625, 17472, 34168, 9129, 581, 26377, 7208, 19759, 14914, 37596, 3730, 15010, 12625, 23373, 26042, 4477, 30687, 33279, 26083, 29040, 13918, 19557, 19759, 37442, 32651, 33923, 29981, 11372, 1545, 18569, 37023, 30687, 6536, 6778, 3730, 5978, 30907, 32249, 30687, 4453, 23373, 26578, 29040, 17114, 12470, 20055, 19759, 29633, 3730, 30687, 5425, 11487, 19759, 18441, 24566, 12525, 3730, 26553, 30687, 33733, 15627, 13612, 13083, 37596, 12625, 20055, 19759, 22189, 26578, 7744, 5425, 26159, 29722, 19759, 3730, 15010, 24187, 4477, 30687, 33279, 26083, 31746, 24032, 32742, 25574, 34805, 15639, 30687, 12879, 4477, 19230, 13433, 17908, 20518, 7921, 17846, 3730, 22189, 1619, 38611, 23373, 30687, 6536, 12625, 9361, 4820, 3386, 26965, 7171, 11960, 28978, 3050, 1542, 30687, 26835, 30687, 25502, 4477, 39456, 12470, 1115, 18643, 3730, 15010, 13714, 9119, 17683, 30458, 33567, 33923, 3868, 30687, 31793, 768, 4477, 29528, 5705, 17683, 3730, 36360, 30118, 13083, 15567, 12625, 3730, 9119, 23373, 30687, 33478, 12470, 3787, 34805, 12232, 13277, 3297, 4792, 17948, 36590, 34805, 14914, 3386, 37361, 27316, 26451, 25146, 37455, 3730, 22368, 39277, 30687, 2265, 19852, 20055, 30687, 4453, 7760, 5228, 13083, 26042, 4477, 30687, 26083, 35799, 16125, 25146, 22153, 13083, 25146, 28018, 15242, 12626, 26578, 544, 32712, 20055, 19759, 26287, 31183, 3386, 29014, 26578, 544, 10706, 30687, 18493, 4477, 30687, 4792, 2782, 19948, 13083, 32742, 23373, 5978, 38611, 4792, 26417, 19759, 14914, 38774, 34663, 5162, 31591, 26083, 34805, 17407, 13277, 3969, 28978, 20518, 5705, 18968, 12625, 27316, 30308, 27869, 27316, 33923, 3050, 26578, 38611, 13881, 37023, 30687, 36285, 30687, 26377, 23136, 28957, 1027, 19557, 33513, 33058, 32742, 8396, 30099, 23136, 12625, 28957, 27316, 22153, 13083, 12625, 33923, 17129, 9129, 20059, 23373, 34663, 33279, 26083, 34663, 30687, 29785, 8539, 11366, 19417, 21629, 30817, 23726, 10103, 27316, 34805, 7436, 13313, 30687, 581, 26377, 26417, 768, 4477, 29528, 13083, 17407, 13277, 4947, 32249, 30687, 4453, 17908, 15442, 9129, 30687, 4792, 30687, 25757, 13083, 39347, 11366, 39664, 17754, 15256, 32903, 36731, 1027, 9128, 33978, 13083, 34805, 12232, 16146, 27316, 19230, 7443, 23373, 28685, 27316, 21508, 16125, 26578, 4453, 35739, 4477, 26451, 32085, 9129, 5978, 30264, 37322, 17539, 21061]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text, label = tf.py_function(encode, \n",
    "                                       inp=[text, label], \n",
    "                                       Tout=(tf.int64, tf.int32))\n",
    "\n",
    "  return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21508 16125 29981  6207 23693   292 31415 33530  3730  6726 33923 21229\n",
      " 31746 27701  5978 29141 11487 26417  4477 30687 28934  5634 29489 39456\n",
      " 33923 39425 11993 13083  9935 30687 23710 25757 16125 30781 23373 25400\n",
      "  1190 13083 10301   734 30687 39065 19557 39456 12625 33923 15004 23136\n",
      " 34663 30687  6807 33622 30687 29489 37023  5978  2782 30687 19229 33622\n",
      " 30687 29489 11366 23037 17908   304 37008 37023 29981 32442 23183  3730\n",
      " 30927 11733 15199 36497 17313 30687 31515 33567 33923 28957 25546  7273\n",
      " 34663 20055 35984  2238 38169 39507 35907  4282 30687  9491 22603 22320\n",
      "  1027 22183 13083 37023 39456 33847 13083 23373 39456  9177 25574  3050\n",
      " 17908 23150 17908 17429 20298 33923 21229 31746 27701 21508 26287 23136\n",
      " 12625 17995 12625 23136  7378 14609 17995 37023 10073 27316 34805  2484\n",
      " 26578 23173  4477 30687 19284 33622 30687  4453 20055 34805 12232 34077\n",
      " 11217 23373 32903 24422  3730  5978 30300 21943  9119 17422 30687 11837\n",
      "  7171  3730  6873 26578 22153 26812 20966 34805 17407 30687 14116 38169\n",
      " 11837 25146 31599 12497  3730 23786 30687 21917 34663 31415  8396 13284\n",
      " 33752 12140  7171 29352 26042 35415 25574 30687 31599 15059 14253 17908\n",
      " 30687  6478 13083 22986 28417 13588 19238 13067 30687  4453 16341 33622\n",
      " 30687 23905 13083 14702 27136 17908 30687 33122 20050 21688 38753 21848\n",
      "  4477  3949 22708 33752 17908  7171 21508 16125 23486  9129  4792 33622\n",
      " 19230   581 26377 12625 19360 30687  4453 37023 15631 14269 35981 32356\n",
      " 35783  4477  2940 29332  2940 13083 16125  4727 19557 29713 39270 13083\n",
      "  3205 30687 14269 35981 17141 26578 33122 32068   190 38483 16125 11277\n",
      " 23486 30687 17141  5279 38169  7171 16105 14474 20960 33622 27316 29785\n",
      " 22189  3050 13083 12625 38713 19360 26578 25262  3050 37023 30687  4453\n",
      "  7760 26578 25262 27316  9092 17407 28957 37023 30687  4453 17908 12625\n",
      " 33923 26578  8396  7427 11277 30687 17141 16125 30579  3730 30687 33168\n",
      " 33622 39206 28126 17472 29981 11240 25502 20055 21508 17129 37023 22694\n",
      " 12625 16169 23373 30687  4893  4477 33279 27409 26083 29222 19759 31050\n",
      "  3730 38774 34663 33279 19759 13910  3730 23786  3730 30687 22993 33279\n",
      " 10398 29528  7378 20055 30687  4453 17472 24948  9129   581 26377 17884\n",
      "  4934 11268 33279 10398 29602 34663 33279 26083  1405  4449  9129 26042\n",
      "  9018 30687 11960  4560 26083 26451 39456 29222 26083 13083 30687  9788\n",
      " 12470 26451 30687 26468 12470 32742 19557 30687  4792  1770 19759 20287\n",
      "  4560 36262 19759 14914 15010 30687  4453 17908 12625 17472 34168  9129\n",
      "   581 26377  7208 19759 14914 37596  3730 15010 12625 23373 26042  4477\n",
      " 30687 33279 26083 29040 13918 19557 19759 37442 32651 33923 29981 11372\n",
      "  1545 18569 37023 30687  6536  6778  3730  5978 30907 32249 30687  4453\n",
      " 23373 26578 29040 17114 12470 20055 19759 29633  3730 30687  5425 11487\n",
      " 19759 18441 24566 12525  3730 26553 30687 33733 15627 13612 13083 37596\n",
      " 12625 20055 19759 22189 26578  7744  5425 26159 29722 19759  3730 15010\n",
      " 24187  4477 30687 33279 26083 31746 24032 32742 25574 34805 15639 30687\n",
      " 12879  4477 19230 13433 17908 20518  7921 17846  3730 22189  1619 38611\n",
      " 23373 30687  6536 12625  9361  4820  3386 26965  7171 11960 28978  3050\n",
      "  1542 30687 26835 30687 25502  4477 39456 12470  1115 18643  3730 15010\n",
      " 13714  9119 17683 30458 33567 33923  3868 30687 31793   768  4477 29528\n",
      "  5705 17683  3730 36360 30118 13083 15567 12625  3730  9119 23373 30687\n",
      " 33478 12470  3787 34805 12232 13277  3297  4792 17948 36590 34805 14914\n",
      "  3386 37361 27316 26451 25146 37455  3730 22368 39277 30687  2265 19852\n",
      " 20055 30687  4453  7760  5228 13083 26042  4477 30687 26083 35799 16125\n",
      " 25146 22153 13083 25146 28018 15242 12626 26578   544 32712 20055 19759\n",
      " 26287 31183  3386 29014 26578   544 10706 30687 18493  4477 30687  4792\n",
      "  2782 19948 13083 32742 23373  5978 38611  4792 26417 19759 14914 38774\n",
      " 34663  5162 31591 26083 34805 17407 13277  3969 28978 20518  5705 18968\n",
      " 12625 27316 30308 27869 27316 33923  3050 26578 38611 13881 37023 30687\n",
      " 36285 30687 26377 23136 28957  1027 19557 33513 33058 32742  8396 30099\n",
      " 23136 12625 28957 27316 22153 13083 12625 33923 17129  9129 20059 23373\n",
      " 34663 33279 26083 34663 30687 29785  8539 11366 19417 21629 30817 23726\n",
      " 10103 27316 34805  7436 13313 30687   581 26377 26417   768  4477 29528\n",
      " 13083 17407 13277  4947 32249 30687  4453 17908 15442  9129 30687  4792\n",
      " 30687 25757 13083 39347 11366 39664 17754 15256 32903 36731  1027  9128\n",
      " 33978 13083 34805 12232 16146 27316 19230  7443 23373 28685 27316 21508\n",
      " 16125 26578  4453 35739  4477 26451 32085  9129  5978 30264 37322 17539\n",
      " 21061]\n",
      "[ 3868  1646  4960 36528 36096 12125  9704  6722 12232 36096 30687 25570\n",
      " 38068  1079 36249 30343  7546  5162 31746 30458 19557 10774 10774 14000\n",
      "  7546 30985  8396 17752 39347 13083 25757 32651 11366 14609 36332 17908\n",
      "  3730 28978  1115 11366  7378 36788 23373 17051 20518 28797 26578 38819\n",
      "  4477 10941 33515 37023 28126  7875 18441 26965 20833  7588 14344 33622\n",
      " 30687 29231 25400  4477  5127 25372  3730 30687 34111  4477 33411 32249\n",
      " 30657  6906 22469  5978  3969 19557 28685 25437 12625 33923  9361 26578\n",
      " 34897 26647 12632 27316 37230  7760 17908 26578 22400   292 12625  7291\n",
      " 12789 19485 18737 34663 10775 14147 13083 25400  5705 24566 37023 27848\n",
      " 13083 12789 18993 12625 16125  3730 19996 17407 26578 26812 30687  4453\n",
      " 30396 13277 29785 37023 32607 15631  4253 13241 10931  1115 35717 20009\n",
      "  9960  2808 31871 16093 35890 18753 30687 36617  2429  7097 35991  7520\n",
      " 20833 30687 11960 35215 29569  1115 20287 36096 36374 26578  3753 12335\n",
      " 25636 28766 12789 22547 35984 16125 16093 33923 35777 26810 16158 19556\n",
      " 17683 26578 13241 31726 13083 14570 17683 26578 24294  2782  4477 28704\n",
      " 33923 23661 36023  3386  5978 17884  2778   292 35984 25115   545  2808\n",
      " 30907  6250 37023  9129 26578 14201 22320 18960 11635 29981 16972  7427\n",
      " 19557 26578  1428 31793 29222 34805 18002 26351 26042  1115 20334 35984\n",
      " 23136 28901 24001 38169 21891 14715 37851  6060 36096 30687  4406 25118\n",
      " 25437 35984 33923  1844 14715 20091  3730 19204  1382 36590 17422 29222\n",
      " 35984 22004 30687 31886 18856  2351 23343  9316 27476  1382 26578  3432\n",
      "  4477 24775 18154 30687  4453  4248 12625 23136 36590 38713 17051 22603\n",
      " 17407  6811  3730 15130 13083 12625 33923 13277 31591 30687 20010  6811\n",
      " 13433 11366  3723 10783  9547  8256 13083 38713 13114 26417  4477 24001\n",
      " 21694 12202 13188 17908 10707  8535 26578  7520 33567  3050  3787  6295\n",
      "  3730  6618 16093 33923 32676 36590 24001 21662 25146 39456 33923 13114\n",
      " 16125 27316 14715 19996 16158 29274 37023  1382 10707 14423 19557 26578\n",
      " 13773  9960 27316  6295  3730 16111  1027 20009 28126 16125 21590 28292\n",
      " 31354 16093 14914 15414  1619  3730 23413 30687 30300 26042  4477 30687\n",
      " 11960 11476  4477 13272 16125  3730  6873 38387 30687 16793 23136  1619\n",
      "  3730 22189 23373 30687  4453 30687 30657 16125 17129 19557 14147 30907\n",
      " 12625 16125  8166 36590 16125 34663 36590  1844 28766 24914 12625 33923\n",
      " 28018 16093 16125 21590  3730 25081 15414 30657 26578 33624 25564  1032\n",
      " 27316  7759  7118 27887 13083 30687 10066  6873 30687 30657 13677 29365\n",
      " 13083 31260 13598 30687 30657 17683 34663 30687 38611 32568 37023  5978\n",
      "  4453   605 31415 29339 13083 23337 30657 11277 16158  3730 17407 26246\n",
      " 13083 26578  4948 13083 31746 30687 29785 12625 27395 26578 26555 13083\n",
      " 19738  5172 19759  1542 38169  2646 12789 35116 17737 39651 29563  3730\n",
      " 29981 26703  9177 14474  4477 30687  4453 11862 28782 17908 12625  7746\n",
      "  9129 30687 30657  3730  6873 19557 30687   292 27869 16093 33923 31448\n",
      " 14080 16125  1844 28766 13083 19525  2227 30687 26812 39131 36590 12625\n",
      " 33923  7977 12789 39032  5978 38950 16125 33622 30687 20287 23786 13083\n",
      " 30687  4453  9361 31993 13114 34663 30687 25636 31193 12429  3109 33931\n",
      " 31746 20018 39149 30657 36834 31917  4282 37929  7941 33624 20041 27316\n",
      " 31183  3386 38432  7171 13083 26578 31149 15325 23733  4477 21161  2780\n",
      " 36316  5494 30687 39165 12625 33923 34663 34168 23373 16226 26268 13083\n",
      " 13277 26042 34061 30657 28943 38713 20055 20518 38774 12625 23373  7588\n",
      " 19468 26382 17501 30687 31149 25262 30687 35321 19557 30687 13773 15325\n",
      " 19996 17422 34805 12141 20518 12884 24566 26671  3730 28126 30687  4710\n",
      " 21237 32651 33923 13277 11476 28766 21161 25146 21173 25723  4820  3386\n",
      "  8485  3730  7000 30687  4975 30687  4453 11037 19000 23695 30907 30687\n",
      "  8999 23520 13083 12625 33923  3050 12789  1134 31431 30687 39065 16125\n",
      " 12625 33923 30687 30308 12625 33923  2616 30118 25262 31746 25262 23373\n",
      "  3560 13277 36307  3050  3787 27316 36590 34663 30687 29789 13433 23695\n",
      " 18753 12869  7088  3730 36327 34805 20027  3710 17778  3135  4477 34663\n",
      " 30687 14524  1405 21709 23373 26578 37944  7378 12625 33923 27659 20526\n",
      " 20055 19759 14858 20518 13364 13523  3730  9252 13083 31650 23373  9252\n",
      " 12625 33923 20950  3730  3710 15743 27316 17737 24566 25079 28766 30657\n",
      " 16125 30687  3787 14269 26480 30687  4453 23136 16125 30687 13294 33923\n",
      " 11960 22015 19852 26578 11758 13083 26578 32855 36590  5978 32356 16125\n",
      " 25705  3925 29222 19759 12141 28766 12625 13083 17683 30687 23905  4477\n",
      " 30687  4453 19759 34285  3386]\n"
     ]
    }
   ],
   "source": [
    "pos_encoded_data = pos_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(pos_encoded_data))[0].numpy()\n",
    "print(example_encoding)\n",
    "\n",
    "neg_encoded_data = neg_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(neg_encoded_data))[0].numpy()\n",
    "print(example_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "TRAIN_AMT = 0.8\n",
    "BATCH_SIZE = 15\n",
    "\n",
    "take_size = math.ceil(len(list(neg_encoded_data)) * (1 - TRAIN_AMT))\n",
    "print(take_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "400\n",
      "107\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "train_data_pos = pos_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "train_data_neg = neg_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "\n",
    "all_labeled_train_data = train_data_pos.concatenate(train_data_neg)\n",
    "train_data = all_labeled_train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "\n",
    "test_data_pos = pos_encoded_data.take(take_size)\n",
    "test_data_neg = neg_encoded_data.take(take_size)\n",
    "all_labeled_test_data = test_data_pos.concatenate(test_data_neg)\n",
    "test_data = all_labeled_test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "\n",
    "print(len(list(train_data_pos)) + len(list(train_data_neg)))\n",
    "print(len(list(test_data_pos)) + len(list(test_data_neg)))\n",
    "train_data_size = len(list(train_data))\n",
    "test_data_size = len(list(test_data))\n",
    "print(train_data_size)\n",
    "print(test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(15, 1397), dtype=int64, numpy=\n",
      "array([[39372, 32742, 13083, ...,     0,     0,     0],\n",
      "       [30687,  8037,  5316, ...,     0,     0,     0],\n",
      "       [32651, 11366, 21848, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [15799, 10903, 31746, ...,     0,     0,     0],\n",
      "       [14609, 12429, 18441, ...,  9528, 19759, 39035],\n",
      "       [37982, 20055, 26042, ...,     0,     0,     0]])>, <tf.Tensor: shape=(15,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "s_train = set()\n",
    "for text, labels in train_data:\n",
    "    s_train.add(labels[0].numpy())\n",
    "    \n",
    "s_test = set()\n",
    "for text, labels in test_data:\n",
    "    s_test.add(labels[0].numpy())\n",
    "print(s_train)\n",
    "print(s_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[21508 16125 29981 ...     0     0     0]\n",
      " [26021 17908 26578 ...     0     0     0]\n",
      " [14442  9129  8695 ...     0     0     0]\n",
      " ...\n",
      " [30687 20480 39000 ...     0     0     0]\n",
      " [30687 26812  4477 ...     0     0     0]\n",
      " [17407 19759 18002 ...     0     0     0]], shape=(15, 1312), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([21508, 16125, 29981, ...,     0,     0,     0]), 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "print(sample_text)\n",
    "sample_text[0].numpy(), sample_labels[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1 # we added 0 for the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "We are going to create 5 word embeddings:\n",
    "\n",
    "1. Bag of words encoding\n",
    "2. Manually trained word embedding on data vocabulary set (Continuous bag of words model)\n",
    "3. Pre-trained Glove 100-dimension embedding\n",
    "4. Pre-trained Glove 300-dimension embedding\n",
    "5. Pre-trained Word2Vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of words\n",
    "\n",
    "Here's an example of how we can use tf.one_hot and bitwise or to create a bag of words encoding for our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 10), dtype=uint32, numpy=\n",
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=uint32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.ops import bitwise_ops\n",
    "test_vocab = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "test_sentences = [[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9],\n",
    "                  [0, 5, 1]]\n",
    "depth = len(test_vocab)\n",
    "\n",
    "def bitwise_or_multiple(tensors):\n",
    "    res = tensors[0]\n",
    "    for i in range(1, len(tensors)):\n",
    "        res = bitwise_ops.bitwise_or(res, tensors[i])\n",
    "    \n",
    "    return res\n",
    "\n",
    "bag_of_words_list = []\n",
    "for sentence in tf.one_hot(test_sentences, depth):\n",
    "    print(sentence)\n",
    "    bag_of_words_list.append(bitwise_or_multiple(tf.cast(sentence, tf.uint32)))\n",
    "\n",
    "bag_of_words = tf.stack(bag_of_words_list)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(15, 39696), dtype=uint32)\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(d, vocab_size):    \n",
    "    bow_list = []\n",
    "    for s in tf.one_hot(d, vocab_size):\n",
    "        bow_list.append(bitwise_or_multiple(tf.cast(s, tf.uint32))[1:])\n",
    "\n",
    "    return tf.stack(bow_list)\n",
    "\n",
    "def bag_of_words_fn(text, labels):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text = tf.py_function(bag_of_words, \n",
    "                                       inp=[text, vocab_size], \n",
    "                                       Tout=tf.uint32)\n",
    "\n",
    "  return encoded_text, labels\n",
    "\n",
    "\n",
    "print(bag_of_words(sample_text, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: This takes a while\n",
    "# train_data_bow = train_data.map(bag_of_words_fn)\n",
    "\n",
    "# sample_text_bow, sample_labels_bow = next(iter(train_data_bow))\n",
    "\n",
    "# print(sample_text_bow)\n",
    "# sample_text_bow[0].numpy(), sample_labels_bow[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_bow = test_data.map(bag_of_words_fn)\n",
    "\n",
    "# sample_text_bow, sample_labels_bow = next(iter(test_data_bow))\n",
    "\n",
    "# print(sample_text_bow)\n",
    "# sample_text_bow[0].numpy(), sample_labels_bow[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Manually Trained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          635168    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 635,457\n",
      "Trainable params: 635,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 4s 37ms/step - loss: 0.7159 - accuracy: 0.5000 - val_loss: 0.6982 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 3s 31ms/step - loss: 0.7146 - accuracy: 0.5000 - val_loss: 0.6902 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 3s 30ms/step - loss: 0.7026 - accuracy: 0.5000 - val_loss: 0.6950 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 3s 32ms/step - loss: 0.7019 - accuracy: 0.5000 - val_loss: 0.6981 - val_accuracy: 0.3333\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 4s 34ms/step - loss: 0.7010 - accuracy: 0.5000 - val_loss: 0.7001 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 5s 42ms/step - loss: 0.6987 - accuracy: 0.5000 - val_loss: 0.7011 - val_accuracy: 0.3333\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 4s 34ms/step - loss: 0.6990 - accuracy: 0.5000 - val_loss: 0.7016 - val_accuracy: 0.3333\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 4s 35ms/step - loss: 0.6999 - accuracy: 0.5000 - val_loss: 0.6978 - val_accuracy: 0.3333\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=test_data, validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
