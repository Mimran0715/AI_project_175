{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "\n",
    "NEG_DIRECTORY_PATH = './review_polarity/txt_sentoken/neg'\n",
    "POS_DIRECTORY_PATH = './review_polarity/txt_sentoken/pos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text Data\n",
    "\n",
    "1. iterate through negative and positive text files\n",
    "\n",
    "2. concat all lines per file to a single string\n",
    "\n",
    "3. create tensor dataset from list of strings\n",
    "\n",
    "4. label tensor dataset with 0 - negative | 1 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_sets = []\n",
    "\n",
    "neg_file_names = list(os.listdir(NEG_DIRECTORY_PATH))\n",
    "pos_file_names = list(os.listdir(POS_DIRECTORY_PATH))\n",
    "\n",
    "lines_list = []\n",
    "for file_name in neg_file_names:\n",
    "  file = open(os.path.join(NEG_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 0))\n",
    "labeled_data_sets.append(labeled_data_set)\n",
    "\n",
    "lines_list = []\n",
    "for file_name in pos_file_names:\n",
    "  file = open(os.path.join(POS_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 1))\n",
    "labeled_data_sets.append(labeled_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "1. Concat positive and negative reviews\n",
    "2. Double check size of full dataset\n",
    "3. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative labeled data len: 1000\n",
      "Negative data len: 1000\n",
      "Positive labeled data len: 1000\n",
      "Positive data len: 1000\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "\n",
    "neg_labeled_data = labeled_data_sets[0]\n",
    "pos_labeled_data = labeled_data_sets[1]\n",
    "print(\"Negative labeled data len:\", len(list(neg_labeled_data)))\n",
    "print(\"Negative data len:\", len(neg_file_names))\n",
    "print(\"Positive labeled data len:\", len(list(pos_labeled_data)))\n",
    "print(\"Positive data len:\", len(pos_file_names))\n",
    "\n",
    "pos_labeled_data = pos_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "neg_labeled_data = neg_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b' \" mercury rising \" has numerous flaws , but there is one that really stands out : its central plot device is unnecessary . that\\'s right . the major aspect of the film , that which is supposed to make it different from other routine government conspiracy / action flicks could be dropped from the beginning , and the movie would turn out exactly the same , if not better . this central device is the fact that a nine-year-old boy is autistic . his name is simon , and an evil government bureaucrat named nicholas kudrow ( alec baldwin ) wants him dead because he unknowingly cracked a supersecret government code slipped into the back of a puzzle magazine by its programmers just to see if someone could beat it . simon is intended to be the heart and soul of the film , and we are supposed to feel for him because he is a poor handicapped child thrown into a violent , unfair world against his will , with only a renegade fbi agent played by bruce willis to protect him . the fact that he is autistic does nothing for the emotional intensity or the plot necessities of \" mercury rising . \" i suppose the original novel upon which the movie was based did much more with this aspect of simon\\'s character and his relationship with willis , but here it is lost . miko hughes , the young actor who plays simon , goes through the prescribed motions of being autistic : he walks slowly , drawls his words , kicks and screams when he\\'s touched , and has a hard time looking at other people . it\\'s a difficult role for an adult , much less a young child , to play , and unfortunately hughes never convinces us that he isn\\'t playing like he\\'s autistic . unlike dustin hoffman\\'s performance in \" rain man \" or leonardo dicaprio\\'s in \" what\\'s eating gilbert grape , \" we are always painfully aware that hughes is acting . however , unlike \" rain man \" which used its character\\'s autism in unique and interesting ways to build a credible and touching story , \" mercury rising \" could go right about its generic , predictable plot with simon being simply a really smart but really shy kid . hell , he doesn\\'t even have to be shy . in fact , the movie might have been more interesting if he had had a more active role , rather than just being carted around under willis\\' arm . willis\\' character , art jeffries , is a disillusioned fbi agent who has been removed from undercover work and is now doing menial tasks like listening to wiretaps with rookies who are happy to be doing anything . he becomes involved with simon when he is called to check out a murder scene at simon\\'s house , where his father has apparently shot his mother in the back and then committed suicide . of course , we know that isn\\'t the way it happened happen because we saw an evil , square-jawed government hitman with a mean-looking crew-cut knock off the parents . simon was able to get away , and jeffries finds him hiding in a secret compartment in a closet , which the rest of the chicago police department had overlooked . jeffries -- who is accused by several characters at different times of being paranoid although his actions never suggest it -- knows there is something more , and he makes it his personal mission to go against everything and everyone in order to protect simon . this is quite a task because that same hitman who knocked off simon\\'s parents is crawling everywhere , attempting to kill simon at the hospital , on the highway , and every other place he goes . late in the movie , jeffries is forced to enlist the aid of a pretty young woman named stacey ( kim dickens ) who he meets at a coffee shop . of all the hard-to-believe aspects of the movie , this is the worst . i can believe in the decency of the human heart , but stacey\\'s character is far too accommodating . not only does she agree to watch simon while jeffries runs off to solve the mystery , she lets him into her apartment at two o\\'clock in the morning when she knows the police is after him , and then lets him leave simon in her apartment which means that she has to forgo a business trip that is desperately needed to pay the rent . the grinding squeals of the rusty plot machine are almost overbearing at this point . the movie might have been redeemed by some good action sequences , but even here \" mercury rising \" doesn\\'t rise to the challenge . the movie was directed by harold becker , who has made some good suspense films including \" sea of love \" ( 1989 ) and \" malice \" ( 1993 ) , but his talent is nowhere to be found in this latest excursion . there is one fight on a streetcar between jeffries and another hitman ( played by peter stormare ) that is so ineptly directed , shot and edited , that i had no idea 1 ) exactly where they were on the streetcar , 2 ) who was hitting who , and 3 ) where this hitman came from and how he knew where jeffries and simon were . the grand finale takes place on the roof of a tall building , and features a harrowing ( yawn ) scene where simon walks along the very edge of the building , not because he has to , but because it\\'s more suspenseful that way . taken as a whole , \" mercury rising \" is an tepid , confused movie that lacks style , wit , and any traces of a sense of humor . usually willis brings his personal brand of understated humor to his roles , but here he is too straight and serious . this is because the movie wants to be an action flick and a heartfelt drama at the same time , but it ends up failing on both fronts . maybe the book was better , but the way it\\'s handled here is a perfect case study in formula filmmaking guaranteed to bore . '>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'allen , star of many a brian depalma movie in the early eighties , has a brief , throwaway part towards the end of \" out of sight , \" as the maid of a crooked financier . in keeping with her past performances , allen wears little more than a green velvet victoria\\'s secret ensemble which begs the question , did they really need a costume designer for this ?  \" out of sight \" is not likely to secure ms . allen the kind of plaudits bestowed upon pam grier , who likewise returned from out of obscurity to appear in another recent elmore leonard adaptation , \" jackie brown , \" but it is intriguing to see her name kicking off the \" also starring \" credits given her limited screen time . the real stars of \" out of sight \" are george clooney ( \" batman & robin \" by way of \" e . r . \" ) and jennifer lopez ( \" selena \" ) , whose winning chemistry , coupled with steven soderbergh\\'s freeze-frame directorial technique , help the film retain the charismatic charm of leonard\\'s original work . with the success of \" get shorty \" three years ago , filmmakers have realized that adapting an elmore leonard novel can be an easier proposition than trying to dream up a storyline from scratch . in \" out of sight , \" clooney stars as jack foley , a career criminal who specializes in unarmed robbery--he just waltzes into banks and politely makes a withdrawal , claiming that his \" partner \" ( just another customer it so happens ) will shoot the manager if the teller doesn\\'t comply . for all of jack\\'s nice-guy charm though , he\\'s not very lucky . his latest scam lands him in florida\\'s glades correctional facility . federal marshal karen sisco ( lopez ) coincidentally arrives at the prison just as jack and his buddy ( ving rhames , typically appealing ) are breaking out . there\\'s a brief struggle and jack bundles himself and karen into the trunk of the getaway car . it\\'s in these cramped confines , pressed intimately together , that jack and karen start falling for one another . the mutual attraction continues as the couple on opposite sides of the law slip in and out of one another\\'s grasp . karen follows jack to detroit , where he\\'s about to pull off another heist , this time a cache of uncut diamonds from incarcerated businessman richard ripley ( played by a toupee-sporting albert brooks , almost unrecognizable in the prison scenes ) . it\\'s during this final caper-gone-awry that allen makes her scant appearance . clooney\\'s range as an actor is limited , but he has undeniable charm . lopez , too , is easy on the eyes , but she demonstrates more depth in her portrayal of karen , a tough yet tender professional . she dispenses with hit men as easily as she out-maneuvers sleazy pick-up artists in a bar . especially cute is a sequence in which karen fantasizes about jack in a motel bathtub . admirably supporting clooney and lopez are don cheadle as fellow felon maurice \\'snoopy\\' miller , steve zahn as the perennially stoned glenn michaels , and dennis farina as karen\\'s dad , who affectionately buys her a piece . add a couple of uncredited cameos from \" jackie brown \" cast members and \" out of sight \" proves to be a lot of fun . not as hip and clever as \" shorty , \" perhaps , or as complex and colorful as \" brown , \" but loads of fun nonetheless . '>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for ex in neg_labeled_data.take(1):\n",
    "  print(ex)\n",
    "print(\"--------------\")\n",
    "for ex in pos_labeled_data.take(1):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode Words\n",
    "\n",
    "1. Get unique vocabulary set among data\n",
    "2. Create encoder based on vocabulary set\n",
    "3. Encode data text -> int using vocabulary as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39696"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in neg_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "for text_tensor, _ in pos_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'allen , star of many a brian depalma movie in the early eighties , has a brief , throwaway part towards the end of \" out of sight , \" as the maid of a crooked financier . in keeping with her past performances , allen wears little more than a green velvet victoria\\'s secret ensemble which begs the question , did they really need a costume designer for this ?  \" out of sight \" is not likely to secure ms . allen the kind of plaudits bestowed upon pam grier , who likewise returned from out of obscurity to appear in another recent elmore leonard adaptation , \" jackie brown , \" but it is intriguing to see her name kicking off the \" also starring \" credits given her limited screen time . the real stars of \" out of sight \" are george clooney ( \" batman & robin \" by way of \" e . r . \" ) and jennifer lopez ( \" selena \" ) , whose winning chemistry , coupled with steven soderbergh\\'s freeze-frame directorial technique , help the film retain the charismatic charm of leonard\\'s original work . with the success of \" get shorty \" three years ago , filmmakers have realized that adapting an elmore leonard novel can be an easier proposition than trying to dream up a storyline from scratch . in \" out of sight , \" clooney stars as jack foley , a career criminal who specializes in unarmed robbery--he just waltzes into banks and politely makes a withdrawal , claiming that his \" partner \" ( just another customer it so happens ) will shoot the manager if the teller doesn\\'t comply . for all of jack\\'s nice-guy charm though , he\\'s not very lucky . his latest scam lands him in florida\\'s glades correctional facility . federal marshal karen sisco ( lopez ) coincidentally arrives at the prison just as jack and his buddy ( ving rhames , typically appealing ) are breaking out . there\\'s a brief struggle and jack bundles himself and karen into the trunk of the getaway car . it\\'s in these cramped confines , pressed intimately together , that jack and karen start falling for one another . the mutual attraction continues as the couple on opposite sides of the law slip in and out of one another\\'s grasp . karen follows jack to detroit , where he\\'s about to pull off another heist , this time a cache of uncut diamonds from incarcerated businessman richard ripley ( played by a toupee-sporting albert brooks , almost unrecognizable in the prison scenes ) . it\\'s during this final caper-gone-awry that allen makes her scant appearance . clooney\\'s range as an actor is limited , but he has undeniable charm . lopez , too , is easy on the eyes , but she demonstrates more depth in her portrayal of karen , a tough yet tender professional . she dispenses with hit men as easily as she out-maneuvers sleazy pick-up artists in a bar . especially cute is a sequence in which karen fantasizes about jack in a motel bathtub . admirably supporting clooney and lopez are don cheadle as fellow felon maurice \\'snoopy\\' miller , steve zahn as the perennially stoned glenn michaels , and dennis farina as karen\\'s dad , who affectionately buys her a piece . add a couple of uncredited cameos from \" jackie brown \" cast members and \" out of sight \" proves to be a lot of fun . not as hip and clever as \" shorty , \" perhaps , or as complex and colorful as \" brown , \" but loads of fun nonetheless . '\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(pos_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5969, 1406, 9040, 24347, 23111, 14778, 13513, 3334, 10552, 19979, 39107, 23080, 15633, 23111, 8197, 35602, 6406, 5554, 19979, 16865, 9040, 14824, 9040, 27544, 30953, 19979, 12792, 9040, 23111, 23746, 13219, 10552, 6318, 26373, 35976, 34539, 6011, 5969, 24420, 8708, 26824, 18488, 23111, 24567, 10157, 35497, 26869, 4443, 19871, 5686, 23825, 19979, 39171, 18588, 3981, 10017, 38883, 23111, 22751, 1207, 19064, 30535, 14824, 9040, 27544, 23642, 33384, 18214, 32041, 10717, 21962, 5969, 19979, 1683, 9040, 35844, 17172, 35444, 28403, 30182, 30592, 2435, 716, 35908, 14824, 9040, 21997, 32041, 3615, 10552, 17846, 37837, 29813, 38123, 901, 4818, 855, 39506, 26051, 23642, 23574, 32041, 34295, 35976, 31051, 26647, 17980, 19979, 32650, 11271, 33080, 13864, 35976, 13957, 28159, 28674, 19979, 5506, 38280, 9040, 14824, 9040, 27544, 1039, 20109, 31098, 14403, 23893, 405, 3312, 9040, 10641, 33459, 18747, 9209, 16936, 25726, 11198, 31168, 19218, 34428, 26373, 32429, 34069, 26869, 2988, 14622, 7323, 29359, 33539, 19979, 25844, 10749, 19979, 9148, 24001, 9040, 38123, 26869, 39688, 15276, 26373, 19979, 23229, 9040, 28634, 38735, 18708, 28469, 23026, 7575, 18577, 20541, 13092, 28609, 8539, 29813, 38123, 15350, 6679, 3175, 8539, 33042, 35300, 18488, 18480, 32041, 23224, 6110, 23111, 19356, 35908, 37097, 10552, 14824, 9040, 27544, 31098, 38280, 30953, 11176, 12495, 23111, 8298, 38026, 30592, 35760, 10552, 2706, 23433, 5260, 28188, 16546, 20838, 35500, 18747, 12352, 5077, 23111, 5521, 30041, 13092, 2994, 8125, 28188, 17846, 1344, 26051, 11265, 16226, 1257, 28289, 19979, 23660, 3983, 19979, 35804, 11934, 25679, 2165, 19064, 36411, 9040, 11176, 26869, 13708, 21664, 24001, 24477, 5260, 26869, 33384, 10052, 5135, 2994, 34888, 1270, 16217, 20442, 10552, 25491, 26869, 27080, 1687, 9258, 12771, 24987, 4409, 4338, 16936, 32663, 26486, 13322, 19979, 39679, 28188, 30953, 11176, 18747, 2994, 6640, 14342, 13894, 16802, 37873, 1039, 5604, 14824, 36448, 26869, 23111, 8197, 2432, 18747, 11176, 35808, 1450, 18747, 4409, 20838, 19979, 35440, 9040, 19979, 6746, 12186, 26051, 26869, 10552, 635, 29806, 84, 30202, 2560, 26885, 13092, 11176, 18747, 4409, 800, 10858, 19064, 14896, 17846, 19979, 33125, 20967, 10715, 30953, 19979, 12693, 29564, 9837, 15445, 9040, 19979, 650, 13268, 10552, 18747, 14824, 9040, 14896, 17846, 26869, 7729, 4409, 17141, 11176, 32041, 22169, 37073, 5260, 26869, 5122, 32041, 7632, 17980, 17846, 8002, 30535, 28674, 23111, 19466, 9040, 26551, 34456, 35908, 23985, 10986, 10189, 8965, 25917, 405, 23111, 7048, 20846, 25035, 39502, 13131, 22304, 10552, 19979, 39679, 10948, 26051, 26869, 23410, 30535, 28627, 10492, 9499, 4361, 13092, 5969, 5077, 35976, 5745, 2120, 31098, 26869, 28192, 30953, 8539, 3214, 23642, 13957, 39506, 5260, 15633, 12836, 24001, 16936, 18634, 23642, 36860, 29564, 19979, 9141, 39506, 36425, 17756, 26824, 29721, 10552, 35976, 32142, 9040, 4409, 23111, 3868, 11408, 21958, 21484, 36425, 5119, 26373, 16366, 22665, 30953, 26555, 30953, 36425, 14824, 4249, 22804, 39249, 6110, 25282, 10552, 23111, 31400, 33513, 7667, 23642, 23111, 38998, 10552, 5686, 4409, 33208, 5122, 11176, 10552, 23111, 37828, 24661, 15786, 8626, 31098, 18747, 16936, 1039, 21094, 12044, 30953, 26958, 3147, 6136, 16219, 31375, 21826, 10299, 30953, 19979, 1853, 28456, 19658, 29492, 18747, 16592, 8069, 30953, 4409, 26869, 12359, 30592, 13540, 25615, 35976, 23111, 2142, 13087, 23111, 12693, 9040, 36209, 29620, 35908, 4818, 855, 19948, 12519, 18747, 14824, 9040, 27544, 26615, 32041, 3175, 23111, 5235, 9040, 33589, 33384, 30953, 37833, 18747, 24952, 30953, 38735, 21885, 30472, 30953, 27662, 18747, 26665, 30953, 855, 39506, 16496, 9040, 33589, 14041]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text, label = tf.py_function(encode, \n",
    "                                       inp=[text, label], \n",
    "                                       Tout=(tf.int64, tf.int32))\n",
    "\n",
    "  return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5969  1406  9040 24347 23111 14778 13513  3334 10552 19979 39107 23080\n",
      " 15633 23111  8197 35602  6406  5554 19979 16865  9040 14824  9040 27544\n",
      " 30953 19979 12792  9040 23111 23746 13219 10552  6318 26373 35976 34539\n",
      "  6011  5969 24420  8708 26824 18488 23111 24567 10157 35497 26869  4443\n",
      " 19871  5686 23825 19979 39171 18588  3981 10017 38883 23111 22751  1207\n",
      " 19064 30535 14824  9040 27544 23642 33384 18214 32041 10717 21962  5969\n",
      " 19979  1683  9040 35844 17172 35444 28403 30182 30592  2435   716 35908\n",
      " 14824  9040 21997 32041  3615 10552 17846 37837 29813 38123   901  4818\n",
      "   855 39506 26051 23642 23574 32041 34295 35976 31051 26647 17980 19979\n",
      " 32650 11271 33080 13864 35976 13957 28159 28674 19979  5506 38280  9040\n",
      " 14824  9040 27544  1039 20109 31098 14403 23893   405  3312  9040 10641\n",
      " 33459 18747  9209 16936 25726 11198 31168 19218 34428 26373 32429 34069\n",
      " 26869  2988 14622  7323 29359 33539 19979 25844 10749 19979  9148 24001\n",
      "  9040 38123 26869 39688 15276 26373 19979 23229  9040 28634 38735 18708\n",
      " 28469 23026  7575 18577 20541 13092 28609  8539 29813 38123 15350  6679\n",
      "  3175  8539 33042 35300 18488 18480 32041 23224  6110 23111 19356 35908\n",
      " 37097 10552 14824  9040 27544 31098 38280 30953 11176 12495 23111  8298\n",
      " 38026 30592 35760 10552  2706 23433  5260 28188 16546 20838 35500 18747\n",
      " 12352  5077 23111  5521 30041 13092  2994  8125 28188 17846  1344 26051\n",
      " 11265 16226  1257 28289 19979 23660  3983 19979 35804 11934 25679  2165\n",
      " 19064 36411  9040 11176 26869 13708 21664 24001 24477  5260 26869 33384\n",
      " 10052  5135  2994 34888  1270 16217 20442 10552 25491 26869 27080  1687\n",
      "  9258 12771 24987  4409  4338 16936 32663 26486 13322 19979 39679 28188\n",
      " 30953 11176 18747  2994  6640 14342 13894 16802 37873  1039  5604 14824\n",
      " 36448 26869 23111  8197  2432 18747 11176 35808  1450 18747  4409 20838\n",
      " 19979 35440  9040 19979  6746 12186 26051 26869 10552   635 29806    84\n",
      " 30202  2560 26885 13092 11176 18747  4409   800 10858 19064 14896 17846\n",
      " 19979 33125 20967 10715 30953 19979 12693 29564  9837 15445  9040 19979\n",
      "   650 13268 10552 18747 14824  9040 14896 17846 26869  7729  4409 17141\n",
      " 11176 32041 22169 37073  5260 26869  5122 32041  7632 17980 17846  8002\n",
      " 30535 28674 23111 19466  9040 26551 34456 35908 23985 10986 10189  8965\n",
      " 25917   405 23111  7048 20846 25035 39502 13131 22304 10552 19979 39679\n",
      " 10948 26051 26869 23410 30535 28627 10492  9499  4361 13092  5969  5077\n",
      " 35976  5745  2120 31098 26869 28192 30953  8539  3214 23642 13957 39506\n",
      "  5260 15633 12836 24001 16936 18634 23642 36860 29564 19979  9141 39506\n",
      " 36425 17756 26824 29721 10552 35976 32142  9040  4409 23111  3868 11408\n",
      " 21958 21484 36425  5119 26373 16366 22665 30953 26555 30953 36425 14824\n",
      "  4249 22804 39249  6110 25282 10552 23111 31400 33513  7667 23642 23111\n",
      " 38998 10552  5686  4409 33208  5122 11176 10552 23111 37828 24661 15786\n",
      "  8626 31098 18747 16936  1039 21094 12044 30953 26958  3147  6136 16219\n",
      " 31375 21826 10299 30953 19979  1853 28456 19658 29492 18747 16592  8069\n",
      " 30953  4409 26869 12359 30592 13540 25615 35976 23111  2142 13087 23111\n",
      " 12693  9040 36209 29620 35908  4818   855 19948 12519 18747 14824  9040\n",
      " 27544 26615 32041  3175 23111  5235  9040 33589 33384 30953 37833 18747\n",
      " 24952 30953 38735 21885 30472 30953 27662 18747 26665 30953   855 39506\n",
      " 16496  9040 33589 14041]\n",
      "[26605 25510 15633 19952 12947 39506 36448 23642 14896 13092 10017 21081\n",
      " 14824 34866  8005 26489 31840 23642 18997 13092 26869 33364 19979 21135\n",
      "  8384  9040 19979 25844 13092  5686 23642 23378 32041  2445 26051 32587\n",
      " 35908   151 19462 16363 14522   862 14084 26283  3175 10020 35908 19979\n",
      "  3086 18747 19979  3334 14309 22438 14824 37043 19979  5834  3983 33384\n",
      " 24050 30535  8005 31840 23642 19979 30879 13092 23111  2236 35340 25257\n",
      " 16629 23642 13912  2994 31051 23642 22103 18747  8539 31216 16363 11872\n",
      " 22828   333  7874 21174 11945 26145 20442  7734 17725  5260  9851 34646\n",
      " 23111 22610 16363 39120  4642 20838 19979  1873  9040 23111 21654 36961\n",
      "   405 34866 21233 28188 32041 34295  3983 32851 26283  7999 26051 22103\n",
      " 23642 34206 32041  3175 19979  9078 18747  5433  9040 19979 25844 18747\n",
      "   806  1039 23378 32041 24639 19064 20442 17725  5260 23642 23111  9848\n",
      " 27072  9629 12023 20838 23111 14393  8952 21275  6466  2994  1257 26373\n",
      "  2184 23111  2084 26289  3583 25917   405  5884 23404 32041  9665 20442\n",
      " 19979 30879 13092  5260 23642 13912   662 18585 19064 19979 13528 33621\n",
      " 30472 19979 26489  7045  9040 26605 25510 10351  1399 19979 39688 15350\n",
      " 35444  5686 19979  3334 36603 22063 18588  2892 26824 26373 30535  8384\n",
      "  9040 22103 26869 33039 18747  2994 29115 26373 23404 39506 37969 26051\n",
      " 23642 37119 24745  8653 19979 11137  3214 30592  2608 22103 34151 17266\n",
      " 19979 36734 25211  9040 38567 13912  5260 29134  5666 26738  2994 12932\n",
      " 10832 18747 21138 22936  5260 26869  2763 18747 15633 23111 23216 28674\n",
      " 19521 13322   151  5585 26051 26869 23111 14008 13981 19064  8539 34982\n",
      "  2892 11452 23111 11137  9629 32041 36255 18747 31825  8653 38268 34903\n",
      " 22009 13092  5260 29247 25679 22961 35194  5260 26869 13912 24913 27954\n",
      "  7816 26869  5369 10552 35191 39188 30472 13461 27588 26869 10552  1857\n",
      " 26869 36502 29295 14133   806  1039 38495 19266 38109 13092  8653 23642\n",
      " 17510 31126 24913 35191 39188  5686 18279 34866 33039 26869 31791 10552\n",
      "  9556 18747 22364 26455 32041  5647 23111 36039 18747 15243 36444 26605\n",
      " 25510 26283 35883 33364  5122 34866 19633  3792 26489 26373 22103 38567\n",
      " 31497 23111 10017   955 39506 10017 17709  4034  7657  5260 11934 25679\n",
      "  4458 18577 32041  3175 17709 10552 30879 19979  3334 15581 18577 17542\n",
      " 26824 22364  3983  5260 21431 21431 23111 26824  6053 13981 22825 18488\n",
      " 28188 38567 21260 34746 14595 23404 14897 23404 33039  6455 29872 23642\n",
      " 23111  7059 26289  3583 30592 15633 17542 22325 35908 16927 15276 18747\n",
      " 23642  5314 27616 17997 34200 35194 17257 32041 21830 26373  2489 30592\n",
      "  1039 27359 32041  3175 27616 27642  5260 27577 11297 26373 22103 22936\n",
      "  5260 23642 32337 32041 15176 14824 23111 14564 21589 13322 22103 26869\n",
      " 35365 37073  2994  1110 15633 26067 17204  2994 26672 10552 19979  1873\n",
      " 18747 15315 35716  2022  9040 26739   806 19559 13092 29247 25679 19979\n",
      "  3312 26051  6675 39264 17725   806 31060  8539 31216 34361 36955 16363\n",
      "  6875 26373 23111  9044 19521 18640  8200 20350 17980 19979 25546 22103\n",
      " 36603  4682 32041 28634 36539 18747 29872  4994 20442 19096 10552 23111\n",
      "  4443 26180 10552 23111  1720  5686 19979 28923  9040 19979  2998 10890\n",
      " 37574 21431 33872 29872 30592 23642 23643   405 13636 30515 13322 32587\n",
      "  2804  9040 38567 35160  3491  2994  2295 38268 16723 26051 27225 36448\n",
      " 23642  4316 26824 18747  5260  5077 26051  2994 29533 25037 32041 35883\n",
      "  6466  1213 18747 36527 10552  3738 32041  9665 22103 30535 23642 24326\n",
      " 23111  5408 17725 13092  5834  6875 30592 28459 17980 22103 26869 25546\n",
      " 23642 25111 16780 17656 32041 10286 22103 13322 19979 11844 29564 19979\n",
      "  2702 18747 23379   151 25502  5260 34151 27229 10552 19979  3334 29872\n",
      " 23642 38035 32041 39405 19979 37500  9040 23111 30855 11137 38469 22828\n",
      " 26956 24349 31928 30592  5260 26846 13322 23111 29584 22648  9040 36411\n",
      " 19979 23216 32041 19810  5653  9040 19979  3334 30535 23642 19979 26385\n",
      " 10351  6679 19810 10552 19979 15775  9040 19979  8855  9078 39506 26956\n",
      " 26869 33039 23642 30876 18634  8776 33384  2184   662 36425  8400 32041\n",
      " 14933 22103 15720 29872 24065 17980 32041 16014 19979  8348 36425  4821\n",
      " 20442 20838 35976 33957 13322 36142 18627 16211 10552 19979 15517 22936\n",
      " 36425 27225 19979 10890 23642 22407 20442 18747 15315  4821 20442 35966\n",
      " 22103 10552 35976 33957  5686  3553 13092 36425 15633 32041 28857 23111\n",
      " 16127  7216 13092 23642 16845 36769 32041 35720 19979  9000 19979 31779\n",
      " 33801  9040 19979 32092 26489  1863  1039 13131 37719 13322 30535 21881\n",
      " 19979  3334 15581 18577 17542 34070   405  3437 25825   862 38407 39506\n",
      "  4458 37969 26605 25510 11934 25679 22090 32041 19979 25005 19979  3334\n",
      " 36603  1374   405  2878  2425 30592 15633 28617  3437 25825 31061 38215\n",
      "  4794 36141  9040 16145  4228 18747  7287  3104 39506  2994  9251 23642\n",
      " 15074 32041  3175 12350 10552 30535 34888  7751 36448 23642 14896  1925\n",
      " 29564 23111  1856 12081 29872 18747 17846  6875 25917   405 19817 12994\n",
      " 13092 23642 11265 11367  1374 17204 18747  9154 13092 10351 21431 13975\n",
      "    87  6936 37043 37073  3981 20144 29564 19979  1856  3812 30592 36603\n",
      " 18036 30592 18747 14255 37073 30535  6875 24823 35908 18747  9774  5260\n",
      " 35576 37073 29872 18747 22103 20144 19979 20714 10409  7183 25502 29564\n",
      " 19979  4472  9040 23111 24951 29621 18747 11058 23111  6707 17905 21589\n",
      " 37073 22103 29134  2404 19979 10052 26008  9040 19979 29621 33384 17725\n",
      "  5260 15633 32041 39506 17725 26051 26869 26824 19976 13092  3312  2693\n",
      " 30953 23111 29540 26605 25510 23642  8539 36864  2932  3334 13092 20145\n",
      "  7847  2361 18747  4484 32995  9040 23111 32191  9040 13597 19567 23404\n",
      " 38659  2994 29533 20221  9040 39451 13597 32041  2994  1230 39506 37969\n",
      "  5260 23642 18634 21309 18747  4960 30535 23642 17725 19979  3334 26145\n",
      " 32041  3175  8539   862 38362 18747 23111  5223 24379 13322 19979  5834\n",
      " 28674 39506 26051 23232  6110 17572 29564 17969 26939 30779 19979 27477\n",
      " 36603 24050 39506 19979  3312 26051 26869 33104 37969 23642 23111 12164\n",
      "  6222 37081 10552  9758 37636 34410 32041 21259]\n"
     ]
    }
   ],
   "source": [
    "pos_encoded_data = pos_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(pos_encoded_data))[0].numpy()\n",
    "print(example_encoding)\n",
    "\n",
    "neg_encoded_data = neg_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(neg_encoded_data))[0].numpy()\n",
    "print(example_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "TRAIN_AMT = 0.8\n",
    "BATCH_SIZE = 15\n",
    "\n",
    "take_size = math.ceil(len(list(neg_encoded_data)) * (1 - TRAIN_AMT))\n",
    "print(take_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "400\n",
      "107\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "train_data_pos = pos_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "train_data_neg = neg_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "\n",
    "all_labeled_train_data = train_data_pos.concatenate(train_data_neg)\n",
    "train_data = all_labeled_train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "\n",
    "test_data_pos = pos_encoded_data.take(take_size)\n",
    "test_data_neg = neg_encoded_data.take(take_size)\n",
    "all_labeled_test_data = test_data_pos.concatenate(test_data_neg)\n",
    "test_data = all_labeled_test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "\n",
    "print(len(list(train_data_pos)) + len(list(train_data_neg)))\n",
    "print(len(list(test_data_pos)) + len(list(test_data_neg)))\n",
    "train_data_size = len(list(train_data))\n",
    "test_data_size = len(list(test_data))\n",
    "print(train_data_size)\n",
    "print(test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(15, 1171), dtype=int64, numpy=\n",
      "array([[ 8463, 33282,  7029, ..., 11137, 30472, 25257],\n",
      "       [30003, 10351, 11362, ...,     0,     0,     0],\n",
      "       [36448, 26869, 23111, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 1201, 38896, 22665, ...,     0,     0,     0],\n",
      "       [38980, 23111, 39668, ...,     0,     0,     0],\n",
      "       [30535, 11556, 23642, ...,     0,     0,     0]])>, <tf.Tensor: shape=(15,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "s_train = set()\n",
    "for text, labels in train_data:\n",
    "    s_train.add(labels[0].numpy())\n",
    "    \n",
    "s_test = set()\n",
    "for text, labels in test_data:\n",
    "    s_test.add(labels[0].numpy())\n",
    "print(s_train)\n",
    "print(s_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 5969  1406  9040 ...     0     0     0]\n",
      " [  189 26489  8847 ...     0     0     0]\n",
      " [30051 23038  1039 ...     0     0     0]\n",
      " ...\n",
      " [23111  3334 13092 ...     0     0     0]\n",
      " [26051 26869 33384 ...     0     0     0]\n",
      " [30953 26373  2994 ...     0     0     0]], shape=(15, 1114), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5969, 1406, 9040, ...,    0,    0,    0]), 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "print(sample_text)\n",
    "sample_text[0].numpy(), sample_labels[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1 # we added 0 for the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "We are going to create 5 word embeddings:\n",
    "\n",
    "1. Bag of words encoding\n",
    "2. Manually trained word embedding on data vocabulary set (Continuous bag of words model)\n",
    "3. Pre-trained Glove 100-dimension embedding\n",
    "4. Pre-trained Glove 300-dimension embedding\n",
    "5. Pre-trained Word2Vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of words\n",
    "\n",
    "Here's an example of how we can use tf.one_hot and bitwise or to create a bag of words encoding for our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(3, 10), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 10), dtype=uint32, numpy=\n",
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=uint32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.ops import bitwise_ops\n",
    "test_vocab = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "test_sentences = [[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9],\n",
    "                  [0, 5, 1]]\n",
    "depth = len(test_vocab)\n",
    "\n",
    "def bitwise_or_multiple(tensors):\n",
    "    res = tensors[0]\n",
    "    for i in range(1, len(tensors)):\n",
    "        res = bitwise_ops.bitwise_or(res, tensors[i])\n",
    "    \n",
    "    return res\n",
    "\n",
    "bag_of_words_list = []\n",
    "for sentence in tf.one_hot(test_sentences, depth):\n",
    "    print(sentence)\n",
    "    bag_of_words_list.append(bitwise_or_multiple(tf.cast(sentence, tf.uint32)))\n",
    "\n",
    "bag_of_words = tf.stack(bag_of_words_list)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(15, 39696), dtype=uint32)\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(d, vocab_size):    \n",
    "    bow_list = []\n",
    "    for s in tf.one_hot(d, vocab_size):\n",
    "        bow_list.append(bitwise_or_multiple(tf.cast(s, tf.uint32))[1:])\n",
    "\n",
    "    return tf.stack(bow_list)\n",
    "\n",
    "def bag_of_words_fn(text, labels):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text = tf.py_function(bag_of_words, \n",
    "                                       inp=[text, vocab_size], \n",
    "                                       Tout=tf.uint32)\n",
    "\n",
    "  return encoded_text, labels\n",
    "\n",
    "\n",
    "print(bag_of_words(sample_text, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: This takes a while\n",
    "# train_data_bow = train_data.map(bag_of_words_fn)\n",
    "\n",
    "# sample_text_bow, sample_labels_bow = next(iter(train_data_bow))\n",
    "\n",
    "# print(sample_text_bow)\n",
    "# sample_text_bow[0].numpy(), sample_labels_bow[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_bow = test_data.map(bag_of_words_fn)\n",
    "\n",
    "# sample_text_bow, sample_labels_bow = next(iter(test_data_bow))\n",
    "\n",
    "# print(sample_text_bow)\n",
    "# sample_text_bow[0].numpy(), sample_labels_bow[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Manually Trained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          635168    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 635,457\n",
      "Trainable params: 635,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15, 1114), dtype=int64, numpy=\n",
       "array([[ 5969,  1406,  9040, ...,     0,     0,     0],\n",
       "       [  189, 26489,  8847, ...,     0,     0,     0],\n",
       "       [30051, 23038,  1039, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [23111,  3334, 13092, ...,     0,     0,     0],\n",
       "       [26051, 26869, 33384, ...,     0,     0,     0],\n",
       "       [30953, 26373,  2994, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "107/107 [==============================] - 4s 37ms/step - loss: 0.7096 - accuracy: 0.5000 - val_loss: 0.7041 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 4s 36ms/step - loss: 0.7092 - accuracy: 0.5000 - val_loss: 0.7021 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 4s 34ms/step - loss: 0.7052 - accuracy: 0.5000 - val_loss: 0.7006 - val_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      " 50/107 [=============>................] - ETA: 0s - loss: 0.6949 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-84babb64803d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     validation_data=test_data, validation_steps=20)\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=test_data, validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
