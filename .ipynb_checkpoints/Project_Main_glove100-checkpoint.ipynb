{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "\n",
    "NEG_DIRECTORY_PATH = './review_polarity/txt_sentoken/neg'\n",
    "POS_DIRECTORY_PATH = './review_polarity/txt_sentoken/pos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text Data\n",
    "\n",
    "1. iterate through negative and positive text files\n",
    "\n",
    "2. concat all lines per file to a single string\n",
    "\n",
    "3. create tensor dataset from list of strings\n",
    "\n",
    "4. label tensor dataset with 0 - negative | 1 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_sets = []\n",
    "\n",
    "neg_file_names = list(os.listdir(NEG_DIRECTORY_PATH))\n",
    "pos_file_names = list(os.listdir(POS_DIRECTORY_PATH))\n",
    "\n",
    "neg_lines_list = []\n",
    "for file_name in neg_file_names:\n",
    "  file = open(os.path.join(NEG_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  neg_lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(neg_lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 0))\n",
    "labeled_data_sets.append(labeled_data_set)\n",
    "\n",
    "pos_lines_list = []\n",
    "for file_name in pos_file_names:\n",
    "  file = open(os.path.join(POS_DIRECTORY_PATH, file_name))\n",
    "  lines = ''\n",
    "  for line in file:\n",
    "    lines += line.rstrip() + ' '\n",
    "  pos_lines_list.append(lines)\n",
    "  file.close()\n",
    "\n",
    "lines_dataset = tf.data.Dataset.from_tensor_slices(pos_lines_list)\n",
    "labeled_data_set = lines_dataset.map(lambda ex: (ex, 1))\n",
    "labeled_data_sets.append(labeled_data_set)\n",
    "\n",
    "#sentences = neg_lines_list + pos_lines_list\n",
    "\n",
    "#with open('/Users/Mal/Downloads/sentences.txt', 'a') as f:\n",
    "#    for s in sentences:\n",
    "#        f.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "1. Concat positive and negative reviews\n",
    "2. Double check size of full dataset\n",
    "3. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((), ()), types: (tf.string, tf.int32)>\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "\n",
    "#all_labeled_data = labeled_data_sets[0].concatenate(labeled_data_sets[1])\n",
    "neg_labeled_data = labeled_data_sets[0]\n",
    "pos_labeled_data = labeled_data_sets[1]\n",
    "print((labeled_data_sets[0]))\n",
    "print(len(neg_file_names))\n",
    "print(len(pos_file_names))\n",
    "#print(len(list(all_labeled_data)))\n",
    "\n",
    "pos_labeled_data = pos_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "neg_labeled_data = neg_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "\n",
    "print(len(list(neg_labeled_data)))\n",
    "print(len(list(pos_labeled_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=4046, shape=(), dtype=string, numpy=b'okay , i just don\\'t know why , but i seem to be getting this diversion to disney-made real-life actors movies . . . as well as real-life acting tim allen movies . i couldn\\'t even make it through \" the santa clause , \" so why did i even see this ?  ( just to make an idle point , i did like \" toy story , \" but that was good ) also , i have this aversion to bad french farces , and if they remake them into american films . well , this is my excuse : it was prom night , i\\'m not a prom person , my best friend and i impulsively went to the drive-ins where they were playing \" grosse pointe blank \" - wouldn\\'t mind seeing it again - but i had to suffer through this first . i agreed to go . ugh . in all fairness , i can say that at least this inane plot wasn\\'t dreamed by an american . it was originally a french film released in america under the pseudonym of \" little indian , big city \" ( french title - \" un indien dans la ville \" ) . i stayed away from it like it was limburgher , and according to roger ebert , that was a good idea . but i can only imagine how bad that must be if this is an improvement . the stupid plot concerns a father who just learns he has a son from his current marriage . let me clarify : his wife ( jobeth williams ) left him years ago , and i mean years - around 13 or so - and went to an island in the carribean or something . he goes to her to finally get the divorce papers signed so he can remarry this . . . thing  ( played with an emphasis on over-done by lolita davidovitch , who is usually good ) . she tells him he has a son as his boatman goes off . he meets him , he has a weird name ( mimi seku , i think . . . or as the bad joke goes - \" mitsubishi \" - laugh track cue ) , he knows english , they fish , more bad jokes , pirhanna joke , the kid has a pet spider , he makes a promise to take him to the statue of liberty . . . you know the drill . now this is where the plot complicates ( well , complicates for this one , at least ) : the fish-out-of-water joke switches from tim allen on an island to his island son in new york city . tim is a stock broker and his coffee profits are plunging because his laptop died and he wasn\\'t able to communicate with his assistant or whatever he is - martin short . a russian mob is tossed into the plot somewhere ( how come in every hokey french import , there is a mob ? ! ) tim learns a lesson of life from his son and we discover that cellular phones can operate on an island even though there are no sockets to recharge the batteries . the story is crap , the jokes are hokey and not really funny , and the actors have to struggle to make it interesting . but the material is so fowl that even a rewrite by quentin tarantino couldn\\'t help it . the whole time , i kept thinking that a grown person had to think this up and several more grown people had to do this . at the end of filming , did they all scream out , \" we\\'ve made a great movie , guys ! \" i sure hope not . big question to get from this film : why would someone want to remake what was billed as one of the worst films of all time if they\\'re just going to do it the exact same way ? huh ? my ( for some of the actors\\' names and a joke or two that made me chuckle , i guess . . . okay , so i really feel bad for it , so i only gave it one star ) '>, <tf.Tensor: id=4047, shape=(), dtype=int32, numpy=0>)\n",
      "--------------\n",
      "(<tf.Tensor: id=4055, shape=(), dtype=string, numpy=b'that thing you do !  ( r ) tom hanks\\'s screenwriting and directorial debut , that thing you do ! , has all the qualities you would associate with the most beloved screen actor of the moment : fun , lively , and oh-so-nice . it is the latter quality , however , that becomes a hindrance , for this \\'60s nostalgia trifle is so nice and sweet that it teeters on becoming bland milquetoast . that thing focuses on the wonders , a teen rock band from erie , pennsylvania that is suddenly thrust into the national spotlight in 1964 when they score a major dance hit called , of course , \" that thing you do ! \" the group\\'s members are , naturally a diverse group : there\\'s brooding lead singer and songwriter jimmy ( johnathon schaech ) ; girl-crazy lenny ( steve zahn ) , the lead guitarist ; a goofy , geeky type known only as the bass player ( ethan embry ) ; and the film\\'s center , guy ( tom everett scott ) , the drummer who has aspirations in jazz . along for the wonders\\' ride to success is jimmy\\'s perpetually neglected galpal , faye ( liv tyler ) . hanks proves to be a capable writer-director , deftly recreating the innocent spirit of 1964 , which hanks calls \" the last innocent year . \" the spirit is not only reflected in the period clothes and settings but also in the music , which , like the other recent period music film , grace of my heart , was expressly written for the film ; hanks himself had a hand in writing four of the tunes--but not the infectious title cut by adam schlesinger , which is guaranteed to stay in your head long after the end credits have rolled ( it\\'s still playing in my mind as i write this ) . it should come as no surprise that hanks the director works well with the actors , eliciting charming , likable work from the entire cast , most notably hanks lookalike scott and tyler , who is remarkable in delivering the film\\'s biggest and best dramatic moment . the work of the young ensemble is so natural that they truly convince as teens of the early \\'60s ; they do not appear to be \\'90s grungers playacting \" retro . \" yet for all the light , frothy charms of that thing you do ! , it\\'s nearly nice to a fault . while this unbridled innocence in film is a refreshing change from all the sinful cinema around these days , there is not enough conflict to keep things consistently interesting . everyone is so happy , basking in the glow of overnight success , marvelling at it all--except toward the end , but even then the tone quickly reverts to sweetness , ending on an appropriately feel-good note . there isn\\'t much of an edge throughout that thing--the only thing that is remotely edgy is hanks\\'s turn as the wonders\\' manager--and thus becomes in danger of being so nice it\\'s bland . but a little niceness goes a long way these days , and there\\'s no denying the entertainment value of that thing you do ! ; it\\'s just about impossible to hate . it\\'s an inoffensive , enjoyable piece of nostalgia that is sure to leave audiences smiling and humming , if not singing , \" that thing you do ! \" --quite possibly for days . to paraphrase a passage from the song : though i try and try to forget that song it is just so hard to do every time they play \" that thing you do ! \" '>, <tf.Tensor: id=4056, shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for ex in neg_labeled_data.take(1):\n",
    "  print(ex)\n",
    "print(\"--------------\")\n",
    "for ex in pos_labeled_data.take(1):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Encode Words\n",
    "\n",
    "1. Get unique vocabulary set among data\n",
    "2. Create encoder based on vocabulary set\n",
    "3. Encode data text -> int using vocabulary as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39696"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "vocab_dict = dict()\n",
    "for text_tensor, _ in neg_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "  #for tk in some_tokens:\n",
    "    #vocab_dict[tk] +=1\n",
    "\n",
    "for text_tensor, _ in pos_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "    \n",
    "count = 0 # 0 reserved for padding - changed to 0 bc of size issue\n",
    "for v in vocabulary_set:\n",
    "    vocab_dict.update({v:count})\n",
    "    count +=1 \n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'that thing you do !  ( r ) tom hanks\\'s screenwriting and directorial debut , that thing you do ! , has all the qualities you would associate with the most beloved screen actor of the moment : fun , lively , and oh-so-nice . it is the latter quality , however , that becomes a hindrance , for this \\'60s nostalgia trifle is so nice and sweet that it teeters on becoming bland milquetoast . that thing focuses on the wonders , a teen rock band from erie , pennsylvania that is suddenly thrust into the national spotlight in 1964 when they score a major dance hit called , of course , \" that thing you do ! \" the group\\'s members are , naturally a diverse group : there\\'s brooding lead singer and songwriter jimmy ( johnathon schaech ) ; girl-crazy lenny ( steve zahn ) , the lead guitarist ; a goofy , geeky type known only as the bass player ( ethan embry ) ; and the film\\'s center , guy ( tom everett scott ) , the drummer who has aspirations in jazz . along for the wonders\\' ride to success is jimmy\\'s perpetually neglected galpal , faye ( liv tyler ) . hanks proves to be a capable writer-director , deftly recreating the innocent spirit of 1964 , which hanks calls \" the last innocent year . \" the spirit is not only reflected in the period clothes and settings but also in the music , which , like the other recent period music film , grace of my heart , was expressly written for the film ; hanks himself had a hand in writing four of the tunes--but not the infectious title cut by adam schlesinger , which is guaranteed to stay in your head long after the end credits have rolled ( it\\'s still playing in my mind as i write this ) . it should come as no surprise that hanks the director works well with the actors , eliciting charming , likable work from the entire cast , most notably hanks lookalike scott and tyler , who is remarkable in delivering the film\\'s biggest and best dramatic moment . the work of the young ensemble is so natural that they truly convince as teens of the early \\'60s ; they do not appear to be \\'90s grungers playacting \" retro . \" yet for all the light , frothy charms of that thing you do ! , it\\'s nearly nice to a fault . while this unbridled innocence in film is a refreshing change from all the sinful cinema around these days , there is not enough conflict to keep things consistently interesting . everyone is so happy , basking in the glow of overnight success , marvelling at it all--except toward the end , but even then the tone quickly reverts to sweetness , ending on an appropriately feel-good note . there isn\\'t much of an edge throughout that thing--the only thing that is remotely edgy is hanks\\'s turn as the wonders\\' manager--and thus becomes in danger of being so nice it\\'s bland . but a little niceness goes a long way these days , and there\\'s no denying the entertainment value of that thing you do ! ; it\\'s just about impossible to hate . it\\'s an inoffensive , enjoyable piece of nostalgia that is sure to leave audiences smiling and humming , if not singing , \" that thing you do ! \" --quite possibly for days . to paraphrase a passage from the song : though i try and try to forget that song it is just so hard to do every time they play \" that thing you do ! \" '\n",
      "--------------------------------\n",
      "b'okay , i just don\\'t know why , but i seem to be getting this diversion to disney-made real-life actors movies . . . as well as real-life acting tim allen movies . i couldn\\'t even make it through \" the santa clause , \" so why did i even see this ?  ( just to make an idle point , i did like \" toy story , \" but that was good ) also , i have this aversion to bad french farces , and if they remake them into american films . well , this is my excuse : it was prom night , i\\'m not a prom person , my best friend and i impulsively went to the drive-ins where they were playing \" grosse pointe blank \" - wouldn\\'t mind seeing it again - but i had to suffer through this first . i agreed to go . ugh . in all fairness , i can say that at least this inane plot wasn\\'t dreamed by an american . it was originally a french film released in america under the pseudonym of \" little indian , big city \" ( french title - \" un indien dans la ville \" ) . i stayed away from it like it was limburgher , and according to roger ebert , that was a good idea . but i can only imagine how bad that must be if this is an improvement . the stupid plot concerns a father who just learns he has a son from his current marriage . let me clarify : his wife ( jobeth williams ) left him years ago , and i mean years - around 13 or so - and went to an island in the carribean or something . he goes to her to finally get the divorce papers signed so he can remarry this . . . thing  ( played with an emphasis on over-done by lolita davidovitch , who is usually good ) . she tells him he has a son as his boatman goes off . he meets him , he has a weird name ( mimi seku , i think . . . or as the bad joke goes - \" mitsubishi \" - laugh track cue ) , he knows english , they fish , more bad jokes , pirhanna joke , the kid has a pet spider , he makes a promise to take him to the statue of liberty . . . you know the drill . now this is where the plot complicates ( well , complicates for this one , at least ) : the fish-out-of-water joke switches from tim allen on an island to his island son in new york city . tim is a stock broker and his coffee profits are plunging because his laptop died and he wasn\\'t able to communicate with his assistant or whatever he is - martin short . a russian mob is tossed into the plot somewhere ( how come in every hokey french import , there is a mob ? ! ) tim learns a lesson of life from his son and we discover that cellular phones can operate on an island even though there are no sockets to recharge the batteries . the story is crap , the jokes are hokey and not really funny , and the actors have to struggle to make it interesting . but the material is so fowl that even a rewrite by quentin tarantino couldn\\'t help it . the whole time , i kept thinking that a grown person had to think this up and several more grown people had to do this . at the end of filming , did they all scream out , \" we\\'ve made a great movie , guys ! \" i sure hope not . big question to get from this film : why would someone want to remake what was billed as one of the worst films of all time if they\\'re just going to do it the exact same way ? huh ? my ( for some of the actors\\' names and a joke or two that made me chuckle , i guess . . . okay , so i really feel bad for it , so i only gave it one star ) '\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(pos_labeled_data))[0].numpy()\n",
    "print(example_text)\n",
    "print(\"--------------------------------\")\n",
    "example_text = next(iter(neg_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4705, 25491, 32229, 4294, 36828, 31295, 25620, 3189, 25491, 16781, 26198, 12679, 33162, 26931, 13858, 26198, 3802, 13258, 38484, 14112, 30198, 3169, 23544, 13129, 23544, 38484, 14112, 22754, 32780, 8021, 3169, 25491, 26173, 36828, 24318, 36189, 35116, 59, 14723, 23647, 27661, 37602, 25620, 16485, 25491, 24318, 1802, 26931, 32229, 26198, 36189, 8637, 33811, 18486, 25491, 16485, 6914, 10596, 31636, 3189, 32233, 8618, 34785, 22049, 25491, 11610, 26931, 9992, 26198, 22712, 22067, 37934, 11475, 8162, 24347, 31294, 5772, 3773, 7426, 14130, 13129, 26931, 39298, 2012, 19456, 35116, 8618, 4607, 33591, 25491, 20861, 32409, 35258, 4607, 24342, 2012, 12607, 10510, 11475, 25491, 6373, 31833, 26198, 14723, 35502, 16376, 4891, 24347, 20653, 38434, 22987, 33934, 9130, 14417, 36828, 17242, 12462, 35116, 1140, 3189, 25491, 30998, 26198, 226, 59, 26931, 2697, 25491, 18619, 26198, 29170, 16018, 10829, 22904, 22985, 25491, 13276, 38651, 32233, 5855, 12958, 26931, 26066, 7579, 6568, 36828, 11016, 13349, 8637, 7426, 35116, 8618, 132, 35258, 22067, 18420, 899, 10829, 27476, 16768, 14723, 10262, 4514, 25842, 18017, 4511, 29819, 22067, 13771, 7273, 18397, 907, 10348, 30964, 25491, 20997, 11633, 18713, 35116, 6914, 35116, 8618, 13733, 11475, 30492, 26198, 10222, 3027, 32233, 8618, 35258, 34785, 1798, 3189, 25491, 13276, 37527, 4529, 29693, 22712, 32233, 34987, 12679, 8162, 26931, 39298, 8637, 8927, 14723, 30149, 7579, 24108, 35258, 3796, 2380, 32229, 9012, 3270, 29642, 35258, 16424, 18713, 33968, 5654, 39680, 32585, 33990, 1305, 33968, 4423, 21411, 21854, 31135, 10855, 34975, 8862, 11475, 25491, 34596, 34975, 11856, 18412, 7532, 37602, 11475, 31833, 26198, 8637, 22068, 10829, 14723, 5573, 7532, 23801, 3270, 369, 26198, 21569, 26198, 32685, 3698, 14723, 10019, 2782, 36460, 37602, 3270, 13276, 15214, 26931, 30376, 29474, 23727, 8637, 2475, 30960, 15574, 35782, 13349, 5268, 39313, 2380, 39298, 7564, 34785, 5421, 12483, 10855, 3270, 29642, 35258, 16424, 23544, 33968, 38971, 369, 3969, 3270, 9127, 10855, 3270, 29642, 35258, 14537, 19333, 34895, 28291, 25491, 20497, 7532, 23544, 14723, 22712, 36886, 369, 25327, 15052, 37719, 12336, 3270, 38720, 17015, 24347, 38775, 12443, 22712, 7335, 8175, 36886, 14723, 33185, 29642, 35258, 17423, 2266, 3270, 10900, 35258, 39106, 26198, 32662, 10855, 26198, 14723, 18089, 4514, 10097, 4246, 31295, 14723, 23067, 34199, 26931, 39298, 4891, 14723, 7579, 29930, 13129, 29930, 23838, 26931, 4194, 5855, 12958, 14723, 38775, 26202, 4514, 26750, 36886, 36399, 18713, 32780, 8021, 30960, 8637, 22068, 26198, 33968, 22068, 16424, 10829, 22287, 3469, 29819, 32780, 39298, 35258, 32744, 25370, 11475, 33968, 20802, 8834, 11166, 10373, 7046, 33968, 6460, 3122, 11475, 3270, 6568, 36828, 13002, 26198, 5483, 23727, 33968, 34303, 7532, 18899, 3270, 39298, 23373, 27830, 35258, 27268, 16609, 39298, 9065, 3773, 14723, 7579, 38141, 29693, 19399, 10829, 148, 1398, 22067, 25512, 17713, 39298, 35258, 16609, 32780, 9012, 35258, 12971, 4514, 14112, 18713, 33968, 16424, 11475, 30099, 10503, 32233, 29050, 12426, 13276, 13058, 30960, 8637, 22068, 24318, 19761, 17713, 11166, 18787, 7197, 26198, 19689, 14723, 2104, 14723, 31636, 39298, 1315, 14723, 7335, 11166, 1398, 11475, 32409, 6531, 25981, 11475, 14723, 30198, 11610, 26198, 34646, 26198, 36189, 35116, 20210, 3189, 14723, 37496, 39298, 37602, 579, 32233, 24318, 35258, 37479, 13349, 31912, 32230, 26173, 36828, 22943, 35116, 14723, 7885, 29933, 25491, 6655, 26904, 32233, 35258, 17395, 24342, 30998, 26198, 20497, 26931, 32005, 11475, 32548, 12443, 17395, 1546, 30998, 26198, 33120, 26931, 5855, 14723, 22437, 4514, 15137, 16485, 24347, 22904, 12492, 26202, 30099, 18167, 13258, 35258, 31485, 35563, 11735, 25491, 35853, 2671, 32409, 4511, 6680, 26198, 3698, 18713, 26931, 18420, 25620, 30548, 19978, 25826, 26198, 31294, 30516, 8618, 7206, 23544, 4194, 4514, 14723, 2812, 14130, 4514, 22904, 29933, 8162, 24347, 33922, 32229, 9285, 26198, 33120, 35116, 14723, 25732, 32094, 30035, 16276, 2012, 23838, 28724, 4514, 14723, 30198, 10354, 11475, 35258, 36886, 7532, 30999, 32233, 13258, 33990, 32105, 25491, 8880, 4705, 37602, 25491, 6531, 29969, 22712, 23838, 35116, 37602, 25491, 37527, 16015, 35116, 4194, 37772]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text, label = tf.py_function(encode, \n",
    "                                       inp=[text, label], \n",
    "                                       Tout=(tf.int64, tf.int32))\n",
    "\n",
    "  return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32233 30376  4246 33120 25590 24135 20889 30674  4448 11475 11860 20881\n",
      " 32233 30376  4246 33120 29642 22904 14723 29079  4246 30548 24046 23727\n",
      " 14723  5873   582 36733 13600  4514 14723 33427  4925  7430 11475  2394\n",
      " 37602 38079 35116 39298 14723 17943 31651 12719 32233  8954 35258 30749\n",
      " 23838 26931 30071 33323 13192 39298 37602 38079 11475 30160 32233 35116\n",
      " 39376 30960 35529  3829 15133 32233 30376 22603 30960 14723 25124 35258\n",
      " 30644 11224 11046 18713 29859  2208 32233 39298   820  5637  3773 14723\n",
      "  8046 31737 10829 17490 18515 24347 29123 35258  7120 14261 22700 22093\n",
      "  4514 30060 32233 30376  4246 33120 14723 21702 30674 13753 11166  3758\n",
      " 35258  8921 21702 17713 30674 32351 13842 31923 11475  3405 13430  9414\n",
      " 21801 27297 20897 22732 28815 11432 14723 13842 38619 35258 39369  6990\n",
      " 15028 24912 37527 23544 14723  1987  8861 15631 17670 11475 14723 18420\n",
      " 30674 30343 14646 24135 36765 18737 14723  8115  2380 29642 39358 10829\n",
      " 11582 15145 23838 14723 25124 24688 26198 38131 39298 13430 30674 25357\n",
      " 12076 25091 15906 12956  2965 20889 25285 26198 12679 35258 21034 31816\n",
      " 19127 37792  2692 14723 11804  9212  4514 17490 29001 20889 14519 14723\n",
      " 10901 11804   279 14723  9212 39298 32409 37527 33370 10829 14723  7780\n",
      " 23114 11475 36435  3189 22049 10829 14723 20217 29001  6914 14723  7224\n",
      " 18247  7780 20217 18420 27306  4514  2012 13003  8618 13297 37671 23838\n",
      " 14723 18420 20889 22221 30998 35258  6217 10829  2589 22869  4514 14723\n",
      " 37103  3189 32409 14723 31087 13771 33243 13349 32113 17522 29001 39298\n",
      " 17945 26198 25177 10829  6706 20002 23115 23512 14723 22437 34966 11610\n",
      "  6143 35116 30674 29950 38434 10829  2012 17242 23544 25491 27325 26931\n",
      " 35116 20769 19399 23544 18787 19965 32233 20889 14723 19127 17533 13129\n",
      " 23727 14723 30198 36211 10178 11921 20507 18713 14723 11424 34873  5873\n",
      " 29700 20889 14316 18737 11475  2965  2380 39298  5722 10829 10397 14723\n",
      " 18420 30674 35365 11475 12607  9072 33427 14723 20507  4514 14723  5086\n",
      "  6971 39298 37602 18750 32233 24347 38721 20882 23544 30059  4514 14723\n",
      " 21178 30071 24347 33120 32409  7029 26198 12679  2945 25505 39515 26697\n",
      " 35224 23838 22904 14723 39418 21247  2347  4514 32233 30376  4246 33120\n",
      " 35116 30674 23900 38079 26198 35258 36813 15180 26931 12996  3471 10829\n",
      " 18420 39298 35258 17902 19432 18713 22904 14723 24306  5460 11856 36429\n",
      "  9236 17713 39298 32409 36483 36731 26198 36907 38828  3674 20210 26657\n",
      " 39298 37602 24319  5093 10829 14723  3326  4514 21583 38131 27946  5855\n",
      " 35116 22904 24526 26266 14723 22437  3189 24318  4886 14723 16599 34512\n",
      " 21082 26198 38340 17002 30960  8637 20586 29969 34785 22470 17713 29584\n",
      " 36828 37136  4514  8637 29644 14045 32233 30376 14723 37527 30376 32233\n",
      " 39298 33138  1503 39298 20889 30674  2263 23544 14723 25124 12814 11475\n",
      "  2884  8954 10829 32768  4514 16339 37602 38079 35116 30674  3829  3189\n",
      " 35258 25842 21708   369 35258 23115 30035 36429  9236 11475 17713 30674\n",
      " 18787 28596 14723 14283 27150  4514 32233 30376  4246 33120 35116 30674\n",
      " 32229 13444 20454 26198 26004 35116 30674  8637 17264 32195 32375  4514\n",
      " 33323 32233 39298 35853 26198 34379 16563 32106 11475 28262  8162 32409\n",
      " 10279 32233 30376  4246 33120 28860 17308 23838  9236 26198 26559 35258\n",
      " 26615 18713 14723 20306 19761 25491 30935 11475 30935 26198 31283 32233\n",
      " 20306 35116 39298 32229 37602 36226 26198 33120   148 29933 24347 11498\n",
      " 32233 30376  4246 33120]\n",
      "[ 4705 25491 32229  4294 36828 31295 25620  3189 25491 16781 26198 12679\n",
      " 33162 26931 13858 26198  3802 13258 38484 14112 30198  3169 23544 13129\n",
      " 23544 38484 14112 22754 32780  8021  3169 25491 26173 36828 24318 36189\n",
      " 35116    59 14723 23647 27661 37602 25620 16485 25491 24318  1802 26931\n",
      " 32229 26198 36189  8637 33811 18486 25491 16485  6914 10596 31636  3189\n",
      " 32233  8618 34785 22049 25491 11610 26931  9992 26198 22712 22067 37934\n",
      " 11475  8162 24347 31294  5772  3773  7426 14130 13129 26931 39298  2012\n",
      " 19456 35116  8618  4607 33591 25491 20861 32409 35258  4607 24342  2012\n",
      " 12607 10510 11475 25491  6373 31833 26198 14723 35502 16376  4891 24347\n",
      " 20653 38434 22987 33934  9130 14417 36828 17242 12462 35116  1140  3189\n",
      " 25491 30998 26198   226    59 26931  2697 25491 18619 26198 29170 16018\n",
      " 10829 22904 22985 25491 13276 38651 32233  5855 12958 26931 26066  7579\n",
      "  6568 36828 11016 13349  8637  7426 35116  8618   132 35258 22067 18420\n",
      "   899 10829 27476 16768 14723 10262  4514 25842 18017  4511 29819 22067\n",
      " 13771  7273 18397   907 10348 30964 25491 20997 11633 18713 35116  6914\n",
      " 35116  8618 13733 11475 30492 26198 10222  3027 32233  8618 35258 34785\n",
      "  1798  3189 25491 13276 37527  4529 29693 22712 32233 34987 12679  8162\n",
      " 26931 39298  8637  8927 14723 30149  7579 24108 35258  3796  2380 32229\n",
      "  9012  3270 29642 35258 16424 18713 33968  5654 39680 32585 33990  1305\n",
      " 33968  4423 21411 21854 31135 10855 34975  8862 11475 25491 34596 34975\n",
      " 11856 18412  7532 37602 11475 31833 26198  8637 22068 10829 14723  5573\n",
      "  7532 23801  3270   369 26198 21569 26198 32685  3698 14723 10019  2782\n",
      " 36460 37602  3270 13276 15214 26931 30376 29474 23727  8637  2475 30960\n",
      " 15574 35782 13349  5268 39313  2380 39298  7564 34785  5421 12483 10855\n",
      "  3270 29642 35258 16424 23544 33968 38971   369  3969  3270  9127 10855\n",
      "  3270 29642 35258 14537 19333 34895 28291 25491 20497  7532 23544 14723\n",
      " 22712 36886   369 25327 15052 37719 12336  3270 38720 17015 24347 38775\n",
      " 12443 22712  7335  8175 36886 14723 33185 29642 35258 17423  2266  3270\n",
      " 10900 35258 39106 26198 32662 10855 26198 14723 18089  4514 10097  4246\n",
      " 31295 14723 23067 34199 26931 39298  4891 14723  7579 29930 13129 29930\n",
      " 23838 26931  4194  5855 12958 14723 38775 26202  4514 26750 36886 36399\n",
      " 18713 32780  8021 30960  8637 22068 26198 33968 22068 16424 10829 22287\n",
      "  3469 29819 32780 39298 35258 32744 25370 11475 33968 20802  8834 11166\n",
      " 10373  7046 33968  6460  3122 11475  3270  6568 36828 13002 26198  5483\n",
      " 23727 33968 34303  7532 18899  3270 39298 23373 27830 35258 27268 16609\n",
      " 39298  9065  3773 14723  7579 38141 29693 19399 10829   148  1398 22067\n",
      " 25512 17713 39298 35258 16609 32780  9012 35258 12971  4514 14112 18713\n",
      " 33968 16424 11475 30099 10503 32233 29050 12426 13276 13058 30960  8637\n",
      " 22068 24318 19761 17713 11166 18787  7197 26198 19689 14723  2104 14723\n",
      " 31636 39298  1315 14723  7335 11166  1398 11475 32409  6531 25981 11475\n",
      " 14723 30198 11610 26198 34646 26198 36189 35116 20210  3189 14723 37496\n",
      " 39298 37602   579 32233 24318 35258 37479 13349 31912 32230 26173 36828\n",
      " 22943 35116 14723  7885 29933 25491  6655 26904 32233 35258 17395 24342\n",
      " 30998 26198 20497 26931 32005 11475 32548 12443 17395  1546 30998 26198\n",
      " 33120 26931  5855 14723 22437  4514 15137 16485 24347 22904 12492 26202\n",
      " 30099 18167 13258 35258 31485 35563 11735 25491 35853  2671 32409  4511\n",
      "  6680 26198  3698 18713 26931 18420 25620 30548 19978 25826 26198 31294\n",
      " 30516  8618  7206 23544  4194  4514 14723  2812 14130  4514 22904 29933\n",
      "  8162 24347 33922 32229  9285 26198 33120 35116 14723 25732 32094 30035\n",
      " 16276  2012 23838 28724  4514 14723 30198 10354 11475 35258 36886  7532\n",
      " 30999 32233 13258 33990 32105 25491  8880  4705 37602 25491  6531 29969\n",
      " 22712 23838 35116 37602 25491 37527 16015 35116  4194 37772]\n"
     ]
    }
   ],
   "source": [
    "pos_encoded_data = pos_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(pos_encoded_data))[0].numpy()\n",
    "#x = next(iter(all_encoded_data))[1].numpy()\n",
    "print(example_encoding)\n",
    "\n",
    "neg_encoded_data = neg_labeled_data.map(encode_map_fn)\n",
    "\n",
    "example_encoding = next(iter(neg_encoded_data))[0].numpy()\n",
    "#x = next(iter(all_encoded_data))[1].numpy()\n",
    "print(example_encoding)\n",
    "#print(x)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "#import numpy as np\n",
    "#embeddings_index = dict()\n",
    "#f = open('/Users/Mal/Downloads/12/model.txt')\n",
    "#f = open('/Users/Mal/Downloads/glove.6B/glove.6B.100d.txt')\n",
    "#for line in f:\n",
    "#\tvalues = line.split()\n",
    "#\tword = values[0]\n",
    "#\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "#\tembeddings_index[word] = coefs\n",
    "#f.close()\n",
    "#print('Loaded %s word vectors.' % len(embeddings_index)) # jason brownlee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mal/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('stunned', 0.852389395236969)\n",
      "[ 2.8572e-01 -1.0165e-03  8.6914e-01 -1.2957e-01 -6.5000e-01  8.9016e-01\n",
      "  2.1256e-01  3.8390e-02  3.1604e-02 -5.7039e-01  1.6348e-01  1.6231e-01\n",
      " -1.3964e-01 -8.7294e-01 -5.0476e-01  8.7020e-02 -8.1818e-01  3.2632e-01\n",
      " -1.8159e-01 -1.5131e-01  3.6507e-01  5.8848e-01  1.3099e-01  2.3497e-02\n",
      "  6.4228e-01 -8.8949e-02  3.7893e-01  5.7788e-01  5.3521e-01 -3.1254e-01\n",
      "  1.4368e-01  1.7315e-01 -1.6229e-01  3.6191e-01 -1.3401e-01 -5.5973e-01\n",
      " -3.1169e-01  2.0823e-01  5.2210e-02 -2.4150e-01 -5.6402e-01 -3.9737e-01\n",
      "  3.2722e-01 -1.9829e-01  3.0287e-01 -1.2222e-01  9.3732e-01  1.0099e+00\n",
      "  4.0845e-01 -4.3619e-01  5.0938e-02 -7.4795e-01 -1.1002e-01  1.9301e-01\n",
      " -4.6331e-01 -1.3785e+00 -5.5050e-01  1.2261e-01  1.7169e-01  2.6167e-01\n",
      " -1.3787e-01  2.7178e-01  1.4616e-01 -2.3679e-01 -1.5203e-01 -1.8593e-01\n",
      " -2.6488e-02  1.1524e+00 -2.7968e-01  2.0679e-01  1.8962e-01  1.0715e-01\n",
      " -2.5267e-01 -1.8774e-01 -5.9221e-02  6.3031e-01  3.7822e-01 -1.8565e-01\n",
      " -3.9893e-01 -4.1508e-02  6.5191e-01  1.8639e-01 -3.8912e-01 -1.8504e-01\n",
      " -8.9510e-01 -1.3592e+00 -4.8301e-01  7.1688e-01 -7.6787e-01 -4.0604e-01\n",
      "  6.0219e-01 -3.3080e-01 -7.7922e-02  6.7711e-01 -5.3339e-01  3.5570e-02\n",
      " -2.4859e-01  4.3234e-01 -3.5514e-01  2.1206e-01]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model_word2vec = api.load(\"glove-wiki-gigaword-100\")\n",
    "sim = model_word2vec.wv.most_similar(\"shocked\")[0]\n",
    "print(sim)\n",
    "print(model_word2vec[sim[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2963\n",
      "(39698, 100)\n",
      "39698\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "count = 0\n",
    "embedding_matrix = np.zeros((encoder.vocab_size, 100))\n",
    "for word, i in vocab_dict.items():\n",
    "    #embedding_vector = embeddings_index.get(word)\n",
    "    #embedding_vector = model_gigaword[word]\n",
    "    #print(embedding_vector.shape)\n",
    "    #if embedding_vector is not None:\n",
    "        #count +=1 \n",
    "        #embedding_matrix[i] = embedding_vector # jason brownlee\n",
    "    #else:\n",
    "    try:\n",
    "        embedding_vector = model_word2vec[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        #print(embedding_vector)\n",
    "        #break\n",
    "    except KeyError:\n",
    "        try:\n",
    "            stem = stemmer.stem(word)\n",
    "            embedding_matrix[i] = model_word2vec[stem]\n",
    "        except KeyError:\n",
    "            try: \n",
    "                stem = stemmer.stem(word)\n",
    "                #sim = model_gigaword.wv.most_similar(stem)[0]\n",
    "                embedding_matrix[i] = model_word2vec[stem]\n",
    "            except KeyError: \n",
    "                count +=1\n",
    "                embedding_matrix[i] = np.zeros(100)            \n",
    "print(count)\n",
    "print(embedding_matrix.shape)\n",
    "print(encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "TRAIN_AMT = 0.8\n",
    "BATCH_SIZE = 15\n",
    "\n",
    "take_size = math.ceil(len(list(pos_encoded_data)) * (1 - TRAIN_AMT))\n",
    "#print(take_size)\n",
    "#take_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=47680, shape=(15, 917), dtype=int64, numpy=\n",
      "array([[26931, 34882, 20232, ...,     0,     0,     0],\n",
      "       [37851, 38790, 34713, ...,     0,     0,     0],\n",
      "       [14723,  5873, 20210, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 3882, 17297,  5725, ..., 10829, 20217,  5387],\n",
      "       [27048,  7579, 11474, ...,     0,     0,     0],\n",
      "       [18598, 11475, 14723, ...,     0,     0,     0]])>, <tf.Tensor: id=47681, shape=(15,), dtype=int32, numpy=array([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0], dtype=int32)>)\n",
      "107\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "train_data_pos = pos_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "train_data_neg = neg_encoded_data.skip(take_size).shuffle(BUFFER_SIZE)\n",
    "\n",
    "#print(type(train_data_pos), type(train_data_neg))\n",
    "all_labeled_train_data = train_data_pos.concatenate(train_data_neg).shuffle(BUFFER_SIZE * 2)\n",
    "#print(all_labeled_data)\n",
    "train_data = all_labeled_train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "for ex in train_data.take(1).take(1):\n",
    "   print(ex)\n",
    "print(len(list(train_data)))\n",
    "\n",
    "test_data_pos = pos_encoded_data.take(take_size)\n",
    "test_data_neg = neg_encoded_data.take(take_size)\n",
    "all_labeled_test_data = test_data_pos.concatenate(test_data_neg).shuffle(BUFFER_SIZE)\n",
    "print(len(list(all_labeled_test_data)))\n",
    "test_data = all_labeled_test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "s_train = set()\n",
    "for text, labels in train_data:\n",
    "    #print(text[0].numpy(), labels[0].numpy())\n",
    "    s_train.add(labels[0].numpy())\n",
    "    \n",
    "s_test = set()\n",
    "for text, labels in test_data:\n",
    "    #print(text[0].numpy(), labels[0].numpy())\n",
    "    s_test.add(labels[0].numpy())\n",
    "print(s_train)\n",
    "print(s_test)\n",
    "#sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "#print(sample_text)\n",
    "#sample_text[0].numpy(), sample_labels[0].numpy()\n",
    "#for i,j in test_data:\n",
    "#    print(i, j)\n",
    "        #print(\"Test Data Tensor:\", i)\n",
    "        #print(\"Test Data Tensor Length:\", len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[36746 23350 30674 ...     0     0     0]\n",
      " [30300 20289 29642 ...     0     0     0]\n",
      " [16485 25491 33120 ...     0     0     0]\n",
      " ...\n",
      " [10829 35303  4514 ...     0     0     0]\n",
      " [35258 26759 34230 ...     0     0     0]\n",
      " [18750   721 16436 ... 23727 13345  5772]], shape=(15, 1917), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([36746, 23350, 30674, ...,     0,     0,     0]), 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "print(sample_text)\n",
    "sample_text[0].numpy(), sample_labels[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         3969800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,971,433\n",
      "Trainable params: 3,971,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# embedding_dim=100\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Embedding(input_dim=encoder.vocab_size, \n",
    "#                                     output_dim=100, \n",
    "#                                     weights=[embedding_matrix], trainable=True),\n",
    "#   tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#   tf.keras.layers.Dense(16, activation='relu'),\n",
    "#   tf.keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 28s 257ms/step - loss: 0.6899 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 23s 211ms/step - loss: 0.6800 - accuracy: 0.5044 - val_loss: 0.6711 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 20s 187ms/step - loss: 0.6601 - accuracy: 0.5587 - val_loss: 0.6681 - val_accuracy: 0.5333\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 24s 221ms/step - loss: 0.6289 - accuracy: 0.6006 - val_loss: 0.6412 - val_accuracy: 0.5667\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 19s 177ms/step - loss: 0.5722 - accuracy: 0.6812 - val_loss: 0.5787 - val_accuracy: 0.6167\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 19s 181ms/step - loss: 0.4841 - accuracy: 0.7594 - val_loss: 0.5234 - val_accuracy: 0.7700\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 26s 241ms/step - loss: 0.3979 - accuracy: 0.8375 - val_loss: 0.4754 - val_accuracy: 0.6967\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 23s 217ms/step - loss: 0.3140 - accuracy: 0.8763 - val_loss: 0.4030 - val_accuracy: 0.8400\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 26s 239ms/step - loss: 0.2386 - accuracy: 0.9156 - val_loss: 0.3864 - val_accuracy: 0.7900\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 21s 200ms/step - loss: 0.1781 - accuracy: 0.9475 - val_loss: 0.3621 - val_accuracy: 0.7933\n"
     ]
    }
   ],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_data,\n",
    "#     epochs=10,\n",
    "#     validation_data=test_data, validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39698, 100)\n",
      "[ 0.04152917  0.11545776 -0.33406228 -0.17455895 -0.26453477  0.01617224\n",
      "  0.551055   -0.14976844  0.5088452   0.21471493  0.32193473 -0.405562\n",
      " -0.0130717   0.28875032  0.30065963  0.64954364 -0.0192679   0.00260546\n",
      "  0.31519017 -0.19620998  0.6611323  -0.32648635 -1.1433882  -0.11372275\n",
      "  0.16442552  0.6150217  -0.37428722  0.00379163 -0.41883495 -0.25914547\n",
      "  0.21996461  0.41267735 -0.11438232  0.2283199   0.08833193 -0.42508495\n",
      " -0.38004413 -0.32378635  0.6237901  -0.42612875  0.79951733  0.62581307\n",
      " -0.13677546  0.03542488  0.6686527   0.27535394 -0.11369078  0.45071808\n",
      " -0.9156967  -0.08972665 -0.66055095  0.9238727  -0.08670236  0.16103148\n",
      "  0.624361    0.07698537  0.01481728  0.07083942 -0.75191337  0.21993494\n",
      "  0.38170215  0.51622975  0.26429248  0.27835718 -0.03203255 -0.14099081\n",
      "  0.73753357 -0.12481198 -0.40891844 -0.39018962  0.09148116 -0.38937125\n",
      "  0.3785054  -0.20824474 -0.67068696  0.45404246  0.09503312  0.5600887\n",
      " -0.4053308   0.07521833  0.737246   -0.32518223 -0.1323771   0.334869\n",
      "  0.5892191  -0.14347076  0.13623217 -0.32058045 -0.10685612 -0.15688205\n",
      " -0.04896904  0.69923687 -0.36568248  0.01966782  0.29221737 -0.38255545\n",
      "  0.06381819  0.2854434  -0.1052528  -0.5876391 ]\n"
     ]
    }
   ],
   "source": [
    "# e = model.layers[0]\n",
    "# weights = e.get_weights()[0]\n",
    "# print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "# print(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39698, 100)\n",
      "39698\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         3969800   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_4 (Separabl (None, None, 64)          6964      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_5 (Separabl (None, None, 64)          4480      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_6 (Separabl (None, None, 128)         8640      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_7 (Separabl (None, None, 128)         17152     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,007,165\n",
      "Trainable params: 37,365\n",
      "Non-trainable params: 3,969,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "107/107 [==============================] - 46s 434ms/step - loss: 0.7088 - accuracy: 0.4994 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/6\n",
      "107/107 [==============================] - 49s 458ms/step - loss: 0.6876 - accuracy: 0.5000 - val_loss: 0.6845 - val_accuracy: 0.5000\n",
      "Epoch 3/6\n",
      "107/107 [==============================] - 44s 409ms/step - loss: 0.5767 - accuracy: 0.8219 - val_loss: 0.5702 - val_accuracy: 0.8500\n",
      "Epoch 4/6\n",
      "107/107 [==============================] - 46s 434ms/step - loss: 0.5185 - accuracy: 0.9719 - val_loss: 0.5771 - val_accuracy: 0.8125\n",
      "Epoch 5/6\n",
      "107/107 [==============================] - 45s 423ms/step - loss: 0.5196 - accuracy: 0.9681 - val_loss: 0.5777 - val_accuracy: 0.8575\n",
      "Epoch 6/6\n",
      "107/107 [==============================] - 45s 423ms/step - loss: 0.5157 - accuracy: 0.9725 - val_loss: 0.5654 - val_accuracy: 0.8625\n",
      "27/27 [==============================] - 3s 124ms/step - loss: 0.5682 - accuracy: 0.8525\n",
      "\n",
      "Eval loss: 0.568, Eval accuracy: 0.853\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.layers import Conv1D, MaxPooling1D\n",
    "#e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "# kernel size = 3 , pooling = 2\n",
    "model = tf.keras.Sequential()\n",
    "##from keras.layers import Dense\n",
    "#from keras.layers import Flatten\n",
    "#from keras.layers import Embedding\n",
    "pool_size = 4\n",
    "filters = 64\n",
    "kernel_size = 5\n",
    "dropout_rate = 0.25\n",
    "blocks = 2\n",
    "print(embedding_matrix.shape)\n",
    "print(encoder.vocab_size)\n",
    "#vocab_size -=2\n",
    "model.add(tf.keras.layers.Embedding(input_dim=encoder.vocab_size, \n",
    "                                    output_dim=100, \n",
    "                                    weights=[embedding_matrix], \n",
    "                                    trainable=False))\n",
    "#model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "#model.add(tf.keras.layers.Flatten(input_shape=(300,)))\n",
    "#model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "#model.add(tf.keras.layers.Flatten())\n",
    "#model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dropout(0.25))\n",
    "# model.add(tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "# model.add(tf.keras.layers.MaxPooling1D(pool_size=pool_size))\n",
    "# #model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50)))\n",
    "# #for units in [50, 50]:\n",
    "# #    model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "# model.add(tf.keras.layers.LSTM(70))\n",
    "# model.add(tf.keras.layers.Dense(2, activation=\"sigmoid\"))\n",
    "# model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "for _ in range(blocks-1):\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate)) # need a large number\n",
    "    model.add(tf.keras.layers.SeparableConv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(tf.keras.layers.SeparableConv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "model.add(tf.keras.layers.SeparableConv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          depthwise_initializer='random_uniform',\n",
    "                          padding='same'))\n",
    "model.add(tf.keras.layers.SeparableConv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          depthwise_initializer='random_uniform',\n",
    "                          padding='same'))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary();\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_data, epochs=6, validation_data=test_data)\n",
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "\n",
    "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# change embedding dim  pretrained - big number -- 100\n",
    "# change model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim), # size of the table, size of the vectors\n",
    "  tf.keras.layers.GlobalAveragePooling1D(), # average result of all my words - length doesnt affect\n",
    "  tf.keras.layers.Dense(16, activation='relu'), # got rid of this --> bag of words + embeddings\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# first none - batch size, second none - words for review, 16 - embedding dim\n",
    "# got rid of 2nd none using global pool 1d\n",
    "# 16 nodes --> cross interaction\n",
    "model.summary() # training the word embeddings by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=test_data, validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "print(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = next(iter(train_data))\n",
    "te = next(iter(test_data))\n",
    "\n",
    "print(\"Train Data Tensor 1:\", tr)\n",
    "print(\"Train Data Tensor 1 Length:\", len(tr))\n",
    "\n",
    "print(\"Test Data Tensor 1:\", te)\n",
    "print(\"Test Data Tensor 1 Length:\", len(te))\n",
    "\n",
    "print(\"Prediction:\", model.predict_classes(te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#embedding_layer = layers.Embedding(1000, 5)\n",
    "##result = embedding_layer(tf.constant([1,2,3]))\n",
    "#result.numpy()\n",
    "#result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "#result.shape\n",
    "for i in test_data:\n",
    "    print(\"Test Data Tensor:\", i)\n",
    "    print(\"Test Data Tensor Length:\", len(i))\n",
    "    print(\"Prediction:\", model.predict_classes(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "     69/Unknown - 416s 6s/step - loss: 0.7000 - accuracy: 0.4961"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-f877c2347bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m history = model.fit(train_data, epochs=3,\n\u001b[1;32m     14\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     validation_steps=30)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=encoder.vocab_size, output_dim=100, weights=[embedding_matrix], trainable=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_data, epochs=3,\n",
    "                    validation_data=test_data,\n",
    "                    validation_steps=30)\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
